{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsPHwagS130i"
      },
      "source": [
        "# PyTorch Advanced: Custom Layers and Deep Architectures\n",
        "\n",
        "## Part 2: Building Complex Operations from Scratch\n",
        "\n",
        "---\n",
        "\n",
        "In Part 1, we learned PyTorch fundamentals, autograd basics, and the high-level nn.Module API. Now we go deeper!\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "| Part | Topic | Key Concepts |\n",
        "|------|-------|-------------|\n",
        "| **I** | Advanced Autograd | Nested autograd, Jacobians, custom gradients |\n",
        "| **II** | Building Ops from Scratch | Convolution, pooling, normalization by hand |\n",
        "| **III** | Custom Layers (Primitives) | Build layers using only torch.nn.Parameter |\n",
        "| **IV** | Custom nn.Module Layers | Proper subclassing with forward() |\n",
        "| **V** | Advanced Architectures | Residual blocks, attention, custom normalizations |\n",
        "| **VI** | Custom Training Loops | Full control over training |\n",
        "| **VII** | Practical Demos | Real-world examples with custom components |\n",
        "\n",
        "---\n",
        "\n",
        "*\"To understand the framework, build it from scratch.\"*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4YYRNzV130l",
        "outputId": "4b77be1b-f057-48d8-f301-097c586b023c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 2.9.0+cpu\n",
            "CUDA Available:  False\n",
            "\n",
            "Using device: cpu\n",
            "\n",
            "Ready for Advanced PyTorch!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                           SETUP & IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Optional, Callable, Union\n",
        "\n",
        "# Beautiful plots\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Check versions and GPU\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"CUDA Available:  {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Device:      {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\\nUsing device: {device}\")\n",
        "\n",
        "# Reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"\\nReady for Advanced PyTorch!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbRlppXM130l"
      },
      "source": [
        "---\n",
        "\n",
        "# Part I: Advanced Autograd Patterns\n",
        "\n",
        "## Beyond Basic Gradient Computation\n",
        "\n",
        "In Part 1, we used autograd for simple gradients. Now we'll explore:\n",
        "\n",
        "- **Nested autograd** for higher-order derivatives\n",
        "- **Jacobian and Hessian** computation\n",
        "- **Custom gradients** with torch.autograd.Function\n",
        "- **Gradient hooks** for manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUeC9-Kg130m",
        "outputId": "a4dfc4fd-4262-4852-f781-5f7cff7427ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "       NESTED AUTOGRAD: HIGHER-ORDER DERIVATIVES\n",
            "============================================================\n",
            "\n",
            "f(x) = x^4, evaluated at x = 2.0\n",
            "\n",
            "f(x)    = 16.0         (expected: 16)\n",
            "f'(x)   = 32.0         (expected: 32 = 4*8)\n",
            "f''(x)  = 48.0         (expected: 48 = 12*4)\n",
            "f'''(x) = 48.0         (expected: 48 = 24*2)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    NESTED AUTOGRAD: HIGHER-ORDER DERIVATIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"       NESTED AUTOGRAD: HIGHER-ORDER DERIVATIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Example: Compute first, second, and third derivatives\n",
        "# f(x) = x^4\n",
        "# f'(x) = 4x^3\n",
        "# f''(x) = 12x^2\n",
        "# f'''(x) = 24x\n",
        "\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "# First derivative\n",
        "y = x ** 4\n",
        "dy_dx = torch.autograd.grad(y, x, create_graph=True)[0]\n",
        "\n",
        "# Second derivative (need create_graph=True to continue differentiating)\n",
        "d2y_dx2 = torch.autograd.grad(dy_dx, x, create_graph=True)[0]\n",
        "\n",
        "# Third derivative\n",
        "d3y_dx3 = torch.autograd.grad(d2y_dx2, x)[0]\n",
        "\n",
        "print(f\"\\nf(x) = x^4, evaluated at x = {x.item()}\")\n",
        "print(f\"\")\n",
        "print(f\"f(x)    = {y.item():.1f}         (expected: 16)\")\n",
        "print(f\"f'(x)   = {dy_dx.item():.1f}         (expected: 32 = 4*8)\")\n",
        "print(f\"f''(x)  = {d2y_dx2.item():.1f}         (expected: 48 = 12*4)\")\n",
        "print(f\"f'''(x) = {d3y_dx3.item():.1f}         (expected: 48 = 24*2)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CSvHZ-3130m",
        "outputId": "8acdb783-7ef3-44da-be8f-4d24b55da824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "              JACOBIAN COMPUTATION\n",
            "============================================================\n",
            "\n",
            "x = [1.0, 2.0, 3.0]\n",
            "y = f(x) = [x1^2, x1*x2, sin(x3)] = [1.0, 2.0, 0.14112000167369843]\n",
            "\n",
            "Jacobian (3x3):\n",
            "[[ 2.         0.         0.       ]\n",
            " [ 2.         1.         0.       ]\n",
            " [ 0.         0.        -0.9899925]]\n",
            "\n",
            "Expected Jacobian:\n",
            "  [2*x1,   0,      0   ]   = [2,   0,     0     ]\n",
            "  [x2,     x1,     0   ]   = [2,   1,     0     ]\n",
            "  [0,      0,  cos(x3) ]   = [0,   0,  -0.9900]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    JACOBIAN COMPUTATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"              JACOBIAN COMPUTATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The Jacobian is the matrix of all first-order partial derivatives\n",
        "# For f: R^n -> R^m, the Jacobian J is m x n where J[i,j] = df_i/dx_j\n",
        "\n",
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "\n",
        "# Vector function: f(x) = [x1^2, x1*x2, sin(x3)]\n",
        "def f(x):\n",
        "    return torch.stack([\n",
        "        x[0] ** 2,\n",
        "        x[0] * x[1],\n",
        "        torch.sin(x[2])\n",
        "    ])\n",
        "\n",
        "# Compute full Jacobian using torch.autograd.functional.jacobian\n",
        "jacobian = torch.autograd.functional.jacobian(f, x)\n",
        "\n",
        "print(f\"\\nx = {x.tolist()}\")\n",
        "print(f\"y = f(x) = [x1^2, x1*x2, sin(x3)] = {f(x).tolist()}\")\n",
        "print(f\"\\nJacobian (3x3):\")\n",
        "print(f\"{jacobian.numpy()}\")\n",
        "\n",
        "print(f\"\\nExpected Jacobian:\")\n",
        "print(f\"  [2*x1,   0,      0   ]   = [2,   0,     0     ]\")\n",
        "print(f\"  [x2,     x1,     0   ]   = [2,   1,     0     ]\")\n",
        "print(f\"  [0,      0,  cos(x3) ]   = [0,   0,  {np.cos(3):.4f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmVwG5QM130m",
        "outputId": "56c8d750-9996-4b96-818d-a114f01418e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "              HESSIAN COMPUTATION\n",
            "============================================================\n",
            "\n",
            "f(x, y) = x^2*y + y^3, at (x, y) = (1.0, 2.0)\n",
            "f = 10.0\n",
            "\n",
            "Gradient: [4.0, 13.0]\n",
            "  Expected: [2xy, x^2 + 3y^2] = [4, 13]\n",
            "\n",
            "Hessian:\n",
            "[[ 4.  2.]\n",
            " [ 2. 12.]]\n",
            "  Expected:\n",
            "  [2y,  2x ]   = [4, 2]\n",
            "  [2x,  6y ]   = [2, 12]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    HESSIAN COMPUTATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"              HESSIAN COMPUTATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The Hessian is the matrix of second-order partial derivatives\n",
        "# H[i,j] = d^2f / (dx_i dx_j)\n",
        "\n",
        "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "\n",
        "# Scalar function: f(x, y) = x^2*y + y^3\n",
        "def scalar_fn(x):\n",
        "    return x[0]**2 * x[1] + x[1]**3\n",
        "\n",
        "# Compute Hessian using torch.autograd.functional.hessian\n",
        "hessian = torch.autograd.functional.hessian(scalar_fn, x)\n",
        "\n",
        "# Also compute gradient for reference\n",
        "f_val = scalar_fn(x)\n",
        "grad = torch.autograd.grad(f_val, x, create_graph=True)[0]\n",
        "\n",
        "print(f\"\\nf(x, y) = x^2*y + y^3, at (x, y) = ({x[0].item()}, {x[1].item()})\")\n",
        "print(f\"f = {f_val.item()}\")\n",
        "print(f\"\\nGradient: {grad.tolist()}\")\n",
        "print(f\"  Expected: [2xy, x^2 + 3y^2] = [4, 13]\")\n",
        "print(f\"\\nHessian:\")\n",
        "print(f\"{hessian.numpy()}\")\n",
        "print(f\"  Expected:\")\n",
        "print(f\"  [2y,  2x ]   = [4, 2]\")\n",
        "print(f\"  [2x,  6y ]   = [2, 12]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1mZBZRH130m",
        "outputId": "458f4ff1-b4f1-45ef-8d7f-c8ed5b96c6a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           CUSTOM AUTOGRAD FUNCTIONS\n",
            "============================================================\n",
            "\n",
            "Input: [3.0, 4.0]\n",
            "Gradient (clipped to norm 1.0): [0.7071067690849304, 0.7071067690849304]\n",
            "Gradient norm: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM AUTOGRAD FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           CUSTOM AUTOGRAD FUNCTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# torch.autograd.Function allows you to define custom forward and backward passes\n",
        "# This is the PyTorch equivalent of TensorFlow's @tf.custom_gradient\n",
        "\n",
        "class ClipGradientNorm(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Forward: identity function\n",
        "    Backward: clip gradient norm\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, clip_value):\n",
        "        # Save clip_value for backward\n",
        "        ctx.clip_value = clip_value\n",
        "        return x.clone()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # Clip the incoming gradient\n",
        "        norm = grad_output.norm()\n",
        "        if norm > ctx.clip_value:\n",
        "            grad_output = grad_output * ctx.clip_value / norm\n",
        "        # Return gradients for x and None for clip_value (not differentiable)\n",
        "        return grad_output, None\n",
        "\n",
        "# Convenience function\n",
        "def clip_gradient_norm(x, clip_value=1.0):\n",
        "    return ClipGradientNorm.apply(x, clip_value)\n",
        "\n",
        "# Test custom gradient\n",
        "x = torch.tensor([3.0, 4.0], requires_grad=True)  # Gradient will have norm 5\n",
        "\n",
        "y = clip_gradient_norm(x, clip_value=1.0)\n",
        "loss = y.sum()\n",
        "loss.backward()\n",
        "\n",
        "print(f\"\\nInput: {x.tolist()}\")\n",
        "print(f\"Gradient (clipped to norm 1.0): {x.grad.tolist()}\")\n",
        "print(f\"Gradient norm: {x.grad.norm().item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NIdVGsk130n",
        "outputId": "49b29fc5-e9b3-4cb1-e398-ed434fbca8ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         STRAIGHT-THROUGH ESTIMATOR\n",
            "============================================================\n",
            "\n",
            "Input:   [0.30000001192092896, 0.699999988079071, 1.2000000476837158, 2.5]\n",
            "Rounded: [0.0, 1.0, 1.0, 2.0]\n",
            "Gradient (straight-through): [0.0, 2.0, 2.0, 4.0]\n",
            "\n",
            " Note: Round is non-differentiable, but we can still train!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    STRAIGHT-THROUGH ESTIMATOR\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"         STRAIGHT-THROUGH ESTIMATOR\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The straight-through estimator is used for:\n",
        "# - Binary/discrete operations that are non-differentiable\n",
        "# - Quantization in neural networks\n",
        "\n",
        "class StraightThroughRound(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Forward: round to nearest integer\n",
        "    Backward: pass gradient through unchanged (identity)\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, x):\n",
        "        return torch.round(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return grad_output  # Straight-through: gradient = identity\n",
        "\n",
        "class StraightThroughSign(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Forward: sign function (-1, 0, or 1)\n",
        "    Backward: gradient of hard tanh (1 if |x| <= 1, else 0)\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, x):\n",
        "        ctx.save_for_backward(x)\n",
        "        return torch.sign(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        x, = ctx.saved_tensors\n",
        "        # Gradient is 1 where |x| <= 1, 0 elsewhere\n",
        "        return grad_output * (x.abs() <= 1).float()\n",
        "\n",
        "# Convenience functions\n",
        "straight_through_round = StraightThroughRound.apply\n",
        "straight_through_sign = StraightThroughSign.apply\n",
        "\n",
        "# Test\n",
        "x = torch.tensor([0.3, 0.7, 1.2, 2.5], requires_grad=True)\n",
        "\n",
        "y = straight_through_round(x)\n",
        "loss = (y ** 2).sum()\n",
        "loss.backward()\n",
        "\n",
        "print(f\"\\nInput:   {x.tolist()}\")\n",
        "print(f\"Rounded: {y.tolist()}\")\n",
        "print(f\"Gradient (straight-through): {x.grad.tolist()}\")\n",
        "print(f\"\\n Note: Round is non-differentiable, but we can still train!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAQA7S-x130n",
        "outputId": "a4750146-df81-4c09-cfcb-6110922447bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "               GRADIENT HOOKS\n",
            "============================================================\n",
            "\n",
            "Gradient logged:\n",
            "  Mean: 0.3841\n",
            "  Std:  1.7298\n",
            "  Norm: 5.3297\n",
            "\n",
            " Hooks are powerful for:\n",
            "  - Debugging gradient flow\n",
            "  - Gradient clipping per-tensor\n",
            "  - Feature visualization (GradCAM)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    GRADIENT HOOKS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"               GRADIENT HOOKS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Hooks allow you to inspect or modify gradients during backward pass\n",
        "# register_hook() on tensors, register_backward_hook() on modules\n",
        "\n",
        "gradient_log = []\n",
        "\n",
        "def gradient_logger_hook(grad):\n",
        "    \"\"\"Hook that logs gradient statistics.\"\"\"\n",
        "    gradient_log.append({\n",
        "        'mean': grad.mean().item(),\n",
        "        'std': grad.std().item(),\n",
        "        'norm': grad.norm().item()\n",
        "    })\n",
        "    return grad  # Return unchanged gradient\n",
        "\n",
        "def gradient_clip_hook(max_norm):\n",
        "    \"\"\"Create a hook that clips gradient norm.\"\"\"\n",
        "    def hook(grad):\n",
        "        norm = grad.norm()\n",
        "        if norm > max_norm:\n",
        "            return grad * max_norm / norm\n",
        "        return grad\n",
        "    return hook\n",
        "\n",
        "# Example: Log gradients during training\n",
        "x = torch.randn(10, requires_grad=True)\n",
        "handle = x.register_hook(gradient_logger_hook)\n",
        "\n",
        "# Forward and backward\n",
        "y = (x ** 2).sum()\n",
        "y.backward()\n",
        "\n",
        "print(f\"\\nGradient logged:\")\n",
        "print(f\"  Mean: {gradient_log[-1]['mean']:.4f}\")\n",
        "print(f\"  Std:  {gradient_log[-1]['std']:.4f}\")\n",
        "print(f\"  Norm: {gradient_log[-1]['norm']:.4f}\")\n",
        "\n",
        "# Remove hook when done\n",
        "handle.remove()\n",
        "\n",
        "print(\"\\n Hooks are powerful for:\")\n",
        "print(\"  - Debugging gradient flow\")\n",
        "print(\"  - Gradient clipping per-tensor\")\n",
        "print(\"  - Feature visualization (GradCAM)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh3MJQUZ130n",
        "outputId": "dda87dbc-e16c-48ba-c4d2-25fa359428bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "            GRADIENT ACCUMULATION\n",
            "============================================================\n",
            "\n",
            "Gradient Accumulation Pattern:\n",
            "  1. optimizer.zero_grad() once at start\n",
            "  2. Scale loss by 1/accumulation_steps\n",
            "  3. loss.backward() accumulates gradients\n",
            "  4. optimizer.step() after all steps\n",
            "  5. Effective batch = mini_batch * N\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    GRADIENT ACCUMULATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"            GRADIENT ACCUMULATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Gradient accumulation is useful when:\n",
        "# - Batch size is too large for GPU memory\n",
        "# - You want effective larger batch sizes\n",
        "\n",
        "def train_with_accumulation(model, data, labels, batch_size, accumulation_steps, optimizer, loss_fn):\n",
        "    \"\"\"\n",
        "    Train with gradient accumulation.\n",
        "    Effective batch size = batch_size * accumulation_steps\n",
        "    \"\"\"\n",
        "    n_samples = len(data)\n",
        "    model.train()\n",
        "\n",
        "    # Zero gradients once at the start\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for step in range(accumulation_steps):\n",
        "        # Get mini-batch\n",
        "        start = (step * batch_size) % n_samples\n",
        "        end = start + batch_size\n",
        "        x_batch = data[start:end]\n",
        "        y_batch = labels[start:end]\n",
        "\n",
        "        # Forward pass\n",
        "        predictions = model(x_batch)\n",
        "        loss = loss_fn(predictions, y_batch)\n",
        "\n",
        "        # Scale loss by accumulation steps (to average gradients)\n",
        "        loss = loss / accumulation_steps\n",
        "\n",
        "        # Backward pass - gradients accumulate!\n",
        "        loss.backward()\n",
        "\n",
        "    # Apply accumulated gradients\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item() * accumulation_steps  # Return unscaled loss\n",
        "\n",
        "print(\"\\nGradient Accumulation Pattern:\")\n",
        "print(\"  1. optimizer.zero_grad() once at start\")\n",
        "print(\"  2. Scale loss by 1/accumulation_steps\")\n",
        "print(\"  3. loss.backward() accumulates gradients\")\n",
        "print(\"  4. optimizer.step() after all steps\")\n",
        "print(\"  5. Effective batch = mini_batch * N\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDFNAZ2Z130o"
      },
      "source": [
        "---\n",
        "\n",
        "# Part II: Building Operations from Scratch\n",
        "\n",
        "## Understanding Neural Network Primitives\n",
        "\n",
        "Before using nn.Module layers, let's understand what they do by building them ourselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW8lDgQj130o",
        "outputId": "0b3c9dc4-6225-496c-f484-d68a14e58162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           CONVOLUTION FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape:  torch.Size([1, 1, 5, 5])\n",
            "Kernel shape: torch.Size([2, 1, 3, 3])\n",
            "Output shape: torch.Size([1, 2, 3, 3])\n",
            "Matches F.conv2d: True\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CONVOLUTION FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           CONVOLUTION FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def conv2d_naive(input_tensor, kernel, stride=1, padding=0):\n",
        "    \"\"\"\n",
        "    Naive 2D convolution implementation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_tensor : tensor (batch, in_channels, height, width)\n",
        "    kernel : tensor (out_channels, in_channels, kernel_h, kernel_w)\n",
        "    stride : int\n",
        "    padding : int\n",
        "    \"\"\"\n",
        "    batch_size, in_channels, in_h, in_w = input_tensor.shape\n",
        "    out_channels, _, k_h, k_w = kernel.shape\n",
        "\n",
        "    # Apply padding\n",
        "    if padding > 0:\n",
        "        input_tensor = F.pad(input_tensor, [padding] * 4)\n",
        "        in_h += 2 * padding\n",
        "        in_w += 2 * padding\n",
        "\n",
        "    out_h = (in_h - k_h) // stride + 1\n",
        "    out_w = (in_w - k_w) // stride + 1\n",
        "\n",
        "    output = torch.zeros(batch_size, out_channels, out_h, out_w)\n",
        "\n",
        "    for i in range(out_h):\n",
        "        for j in range(out_w):\n",
        "            h_start = i * stride\n",
        "            w_start = j * stride\n",
        "            # Extract patch: (batch, in_channels, k_h, k_w)\n",
        "            patch = input_tensor[:, :, h_start:h_start+k_h, w_start:w_start+k_w]\n",
        "\n",
        "            # Convolve: sum over (in_channels, k_h, k_w)\n",
        "            # patch: (batch, in_c, k_h, k_w)\n",
        "            # kernel: (out_c, in_c, k_h, k_w)\n",
        "            # output: (batch, out_c)\n",
        "            conv = torch.einsum('bihw,oihw->bo', patch, kernel)\n",
        "            output[:, :, i, j] = conv\n",
        "\n",
        "    return output\n",
        "\n",
        "# Test our implementation\n",
        "x = torch.randn(1, 1, 5, 5)  # 1 image, 1 channel, 5x5\n",
        "kernel = torch.randn(2, 1, 3, 3)  # 2 output channels, 1 input channel, 3x3\n",
        "\n",
        "our_output = conv2d_naive(x, kernel, stride=1, padding=0)\n",
        "torch_output = F.conv2d(x, kernel, stride=1, padding=0)\n",
        "\n",
        "print(f\"\\nInput shape:  {x.shape}\")\n",
        "print(f\"Kernel shape: {kernel.shape}\")\n",
        "print(f\"Output shape: {our_output.shape}\")\n",
        "print(f\"Matches F.conv2d: {torch.allclose(our_output, torch_output, atol=1e-5)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoqcFXo3130o",
        "outputId": "7c9576cf-7853-4f4f-f7ca-c92aea312d3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           MAX POOLING FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape: torch.Size([1, 1, 4, 4])\n",
            "Input:\n",
            "[[ 1.  2.  3.  4.]\n",
            " [ 5.  6.  7.  8.]\n",
            " [ 9. 10. 11. 12.]\n",
            " [13. 14. 15. 16.]]\n",
            "\n",
            "Output shape: torch.Size([1, 1, 2, 2])\n",
            "Output:\n",
            "[[ 6.  8.]\n",
            " [14. 16.]]\n",
            "\n",
            "Matches F.max_pool2d: True\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    MAX POOLING FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           MAX POOLING FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def max_pool2d_naive(input_tensor, pool_size=2, stride=2):\n",
        "    \"\"\"\n",
        "    Naive max pooling implementation.\n",
        "\n",
        "    For each pool_size x pool_size window, take the maximum.\n",
        "    \"\"\"\n",
        "    batch_size, channels, in_h, in_w = input_tensor.shape\n",
        "\n",
        "    out_h = (in_h - pool_size) // stride + 1\n",
        "    out_w = (in_w - pool_size) // stride + 1\n",
        "\n",
        "    output = torch.zeros(batch_size, channels, out_h, out_w)\n",
        "\n",
        "    for i in range(out_h):\n",
        "        for j in range(out_w):\n",
        "            h_start = i * stride\n",
        "            w_start = j * stride\n",
        "            # Extract window\n",
        "            window = input_tensor[:, :, h_start:h_start+pool_size,\n",
        "                                  w_start:w_start+pool_size]\n",
        "            # Max over spatial dimensions\n",
        "            output[:, :, i, j] = window.amax(dim=(2, 3))\n",
        "\n",
        "    return output\n",
        "\n",
        "# Test\n",
        "x = torch.tensor([[[[1., 2., 3., 4.],\n",
        "                    [5., 6., 7., 8.],\n",
        "                    [9., 10., 11., 12.],\n",
        "                    [13., 14., 15., 16.]]]])\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"Input:\")\n",
        "print(x[0, 0].numpy())\n",
        "\n",
        "our_pool = max_pool2d_naive(x, pool_size=2, stride=2)\n",
        "torch_pool = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "\n",
        "print(f\"\\nOutput shape: {our_pool.shape}\")\n",
        "print(f\"Output:\")\n",
        "print(our_pool[0, 0].numpy())\n",
        "print(f\"\\nMatches F.max_pool2d: {torch.allclose(our_pool, torch_pool)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLm27c7q130o",
        "outputId": "34a8bc16-1340-46f1-915e-f23d196895e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        BATCH NORMALIZATION FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape: torch.Size([8, 4])\n",
            "Input mean per feature: [0.5206222534179688, -0.17970936000347137, 0.45833995938301086, 0.2649056315422058]\n",
            "Input std per feature:  [0.3801516592502594, 1.219132661819458, 0.4272221326828003, 0.585858941078186]\n",
            "\n",
            "Output (training) mean: ['-0.0000', '0.0000', '0.0000', '0.0000']\n",
            "Output (training) std:  ['1.0690', '1.0690', '1.0690', '1.0690']\n",
            "\n",
            " After BatchNorm, each feature has ~0 mean and ~1 std!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    BATCH NORMALIZATION FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        BATCH NORMALIZATION FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class BatchNormFromScratch:\n",
        "    \"\"\"\n",
        "    Batch Normalization implemented from scratch.\n",
        "\n",
        "    During training:\n",
        "        x_norm = (x - batch_mean) / sqrt(batch_var + epsilon)\n",
        "        y = gamma * x_norm + beta\n",
        "\n",
        "    During inference:\n",
        "        Use running mean and variance instead of batch statistics.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_features, epsilon=1e-5, momentum=0.1):\n",
        "        self.epsilon = epsilon\n",
        "        self.momentum = momentum\n",
        "\n",
        "        # Learnable parameters\n",
        "        self.gamma = torch.ones(num_features, requires_grad=True)\n",
        "        self.beta = torch.zeros(num_features, requires_grad=True)\n",
        "\n",
        "        # Running statistics (not trainable)\n",
        "        self.running_mean = torch.zeros(num_features)\n",
        "        self.running_var = torch.ones(num_features)\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        if training:\n",
        "            # Compute batch statistics\n",
        "            batch_mean = x.mean(dim=0)\n",
        "            batch_var = x.var(dim=0, unbiased=False)\n",
        "\n",
        "            # Update running statistics (detached, no gradient)\n",
        "            with torch.no_grad():\n",
        "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
        "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
        "\n",
        "            mean, var = batch_mean, batch_var\n",
        "        else:\n",
        "            mean, var = self.running_mean, self.running_var\n",
        "\n",
        "        # Normalize\n",
        "        x_norm = (x - mean) / torch.sqrt(var + self.epsilon)\n",
        "\n",
        "        # Scale and shift\n",
        "        return self.gamma * x_norm + self.beta\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "# Test\n",
        "bn = BatchNormFromScratch(num_features=4)\n",
        "x = torch.randn(8, 4)  # Batch of 8, 4 features\n",
        "\n",
        "y_train = bn(x, training=True)\n",
        "y_eval = bn(x, training=False)\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"Input mean per feature: {x.mean(dim=0).tolist()}\")\n",
        "print(f\"Input std per feature:  {x.std(dim=0).tolist()}\")\n",
        "print(f\"\\nOutput (training) mean: {[f'{v:.4f}' for v in y_train.mean(dim=0).tolist()]}\")\n",
        "print(f\"Output (training) std:  {[f'{v:.4f}' for v in y_train.std(dim=0).tolist()]}\")\n",
        "print(f\"\\n After BatchNorm, each feature has ~0 mean and ~1 std!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0QylO2n130o",
        "outputId": "578695d6-b0ba-4183-e2a6-cd0131f9a3d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        LAYER NORMALIZATION FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape: torch.Size([2, 3, 4])\n",
            "\n",
            "For sample [0, 0, :]:\n",
            "  Input:  [-0.16911523044109344, 1.931160569190979, 1.0118638277053833, -1.4364064931869507]\n",
            "  Output: ['-0.3981', '1.2626', '0.5357', '-1.4002']\n",
            "  Output mean: 0.000000\n",
            "  Output std:  1.1547\n",
            "\n",
            " Key difference:\n",
            "  BatchNorm: normalize across batch (for each feature)\n",
            "  LayerNorm: normalize across features (for each sample)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    LAYER NORMALIZATION FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        LAYER NORMALIZATION FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class LayerNormFromScratch:\n",
        "    \"\"\"\n",
        "    Layer Normalization: Normalize across features (not batch).\n",
        "\n",
        "    Used in Transformers because:\n",
        "    - Works with any batch size (including 1)\n",
        "    - No running statistics needed\n",
        "    - Each sample normalized independently\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, normalized_shape, epsilon=1e-5):\n",
        "        self.epsilon = epsilon\n",
        "        self.normalized_shape = normalized_shape\n",
        "\n",
        "        # Learnable parameters\n",
        "        self.gamma = torch.ones(normalized_shape, requires_grad=True)\n",
        "        self.beta = torch.zeros(normalized_shape, requires_grad=True)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # Compute statistics across last dimensions\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "\n",
        "        # Normalize\n",
        "        x_norm = (x - mean) / torch.sqrt(var + self.epsilon)\n",
        "\n",
        "        # Scale and shift\n",
        "        return self.gamma * x_norm + self.beta\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "# Test\n",
        "ln = LayerNormFromScratch(normalized_shape=4)\n",
        "x = torch.randn(2, 3, 4)  # (batch, seq, features)\n",
        "\n",
        "y = ln(x)\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"\\nFor sample [0, 0, :]:\")\n",
        "print(f\"  Input:  {x[0, 0, :].tolist()}\")\n",
        "print(f\"  Output: {[f'{v:.4f}' for v in y[0, 0, :].tolist()]}\")\n",
        "print(f\"  Output mean: {y[0, 0, :].mean().item():.6f}\")\n",
        "print(f\"  Output std:  {y[0, 0, :].std().item():.4f}\")\n",
        "\n",
        "print(f\"\\n Key difference:\")\n",
        "print(f\"  BatchNorm: normalize across batch (for each feature)\")\n",
        "print(f\"  LayerNorm: normalize across features (for each sample)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlXqtm1V130o",
        "outputId": "fdbd299c-0041-496e-bee6-7ce3c3874c7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "            DROPOUT FROM SCRATCH\n",
            "============================================================\n",
            "Input: all ones, shape torch.Size([2, 10])\n",
            "\n",
            "Dropout sample 1: [0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0]\n",
            "Dropout sample 2: [2.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 2.0, 0.0]\n",
            "Dropout sample 3: [2.0, 2.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 0.0]\n",
            "\n",
            "During inference (training=False):\n",
            "  Output: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "\n",
            "Average over 1000 samples: 1.0055 (should be ~1.0)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    DROPOUT FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"            DROPOUT FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def dropout_from_scratch(x, rate=0.5, training=True):\n",
        "    \"\"\"\n",
        "    Dropout: Randomly zero out neurons during training.\n",
        "\n",
        "    Key insight: Scale by 1/(1-rate) during training so that\n",
        "    expected value remains the same during inference.\n",
        "    \"\"\"\n",
        "    if not training or rate == 0:\n",
        "        return x\n",
        "\n",
        "    # Create random mask\n",
        "    keep_prob = 1 - rate\n",
        "    mask = (torch.rand_like(x) < keep_prob).float()\n",
        "\n",
        "    # Apply mask and scale (inverted dropout)\n",
        "    return (x * mask) / keep_prob\n",
        "\n",
        "# Test\n",
        "x = torch.ones(2, 10)\n",
        "\n",
        "print(f\"Input: all ones, shape {x.shape}\")\n",
        "print(f\"\")\n",
        "\n",
        "# Multiple dropout samples\n",
        "for i in range(3):\n",
        "    dropped = dropout_from_scratch(x, rate=0.5, training=True)\n",
        "    print(f\"Dropout sample {i+1}: {dropped[0].tolist()}\")\n",
        "\n",
        "print(f\"\\nDuring inference (training=False):\")\n",
        "print(f\"  Output: {dropout_from_scratch(x, rate=0.5, training=False)[0].tolist()}\")\n",
        "\n",
        "# Verify expected value is preserved\n",
        "samples = torch.stack([dropout_from_scratch(x, rate=0.5) for _ in range(1000)])\n",
        "print(f\"\\nAverage over 1000 samples: {samples.mean().item():.4f} (should be ~1.0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eXTxywF130o"
      },
      "source": [
        "---\n",
        "\n",
        "# Part III: Custom Layers Using Primitives\n",
        "\n",
        "## Building Layers with nn.Parameter Only\n",
        "\n",
        "Before using PyTorch's layer classes, let's build fully functional layers using only basic operations. This shows exactly what happens under the hood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVIR_S55130o",
        "outputId": "390eb83d-6cc1-4d4e-f7b5-066fdfdf4593"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          DENSE LAYER FROM PRIMITIVES\n",
            "============================================================\n",
            "\n",
            "DenseLayerPrimitive(4, 3, activation='relu')\n",
            "Input shape:  torch.Size([2, 4])\n",
            "Output shape: torch.Size([2, 3])\n",
            "Weight shape: torch.Size([4, 3])\n",
            "Bias shape:   torch.Size([3])\n",
            "\n",
            "Output:\n",
            "[[2.5953684  0.         2.304854  ]\n",
            " [0.         0.66387445 0.        ]]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    DENSE LAYER FROM PRIMITIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          DENSE LAYER FROM PRIMITIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class DenseLayerPrimitive:\n",
        "    \"\"\"\n",
        "    Fully connected layer using only basic PyTorch operations.\n",
        "\n",
        "    Mathematically: y = activation(x @ W + b)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, activation=None, use_bias=True):\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        # Select activation\n",
        "        self.activation = {\n",
        "            None: lambda x: x,\n",
        "            'relu': F.relu,\n",
        "            'sigmoid': torch.sigmoid,\n",
        "            'tanh': torch.tanh,\n",
        "            'softmax': lambda x: F.softmax(x, dim=-1)\n",
        "        }.get(activation, activation)  # Allow passing functions directly\n",
        "\n",
        "        # He initialization for weights\n",
        "        stddev = np.sqrt(2.0 / in_features)\n",
        "        self.W = torch.randn(in_features, out_features, requires_grad=True) * stddev\n",
        "\n",
        "        if use_bias:\n",
        "            self.b = torch.zeros(out_features, requires_grad=True)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Forward pass: y = activation(x @ W + b)\"\"\"\n",
        "        out = x @ self.W\n",
        "        if self.use_bias:\n",
        "            out = out + self.b\n",
        "        return self.activation(out)\n",
        "\n",
        "    def parameters(self):\n",
        "        if self.use_bias:\n",
        "            return [self.W, self.b]\n",
        "        return [self.W]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"DenseLayerPrimitive({self.in_features}, {self.out_features})\"\n",
        "\n",
        "# Test\n",
        "dense = DenseLayerPrimitive(4, 3, activation='relu')\n",
        "x = torch.randn(2, 4)\n",
        "y = dense(x)\n",
        "\n",
        "print(f\"\\nDenseLayerPrimitive(4, 3, activation='relu')\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Weight shape: {dense.W.shape}\")\n",
        "print(f\"Bias shape:   {dense.b.shape}\")\n",
        "print(f\"\\nOutput:\\n{y.detach().numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uM5Lo_Ci130p",
        "outputId": "a89e1ffa-96be-4f23-c55c-f457a9afc6fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          CONV2D LAYER FROM PRIMITIVES\n",
            "============================================================\n",
            "\n",
            "Conv2DLayerPrimitive(3, 16, kernel_size=3, padding=1)\n",
            "Input shape:  torch.Size([1, 3, 28, 28])\n",
            "Output shape: torch.Size([1, 16, 28, 28])\n",
            "Kernel shape: torch.Size([16, 3, 3, 3])\n",
            "Parameters:   448\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CONV2D LAYER FROM PRIMITIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          CONV2D LAYER FROM PRIMITIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class Conv2DLayerPrimitive:\n",
        "    \"\"\"\n",
        "    2D Convolutional layer using only basic operations and F.conv2d.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding=0, activation=None, use_bias=True):\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        # Handle kernel_size as int or tuple\n",
        "        if isinstance(kernel_size, int):\n",
        "            kernel_size = (kernel_size, kernel_size)\n",
        "\n",
        "        # Select activation\n",
        "        self.activation = {\n",
        "            None: lambda x: x,\n",
        "            'relu': F.relu,\n",
        "            'sigmoid': torch.sigmoid,\n",
        "            'tanh': torch.tanh,\n",
        "        }.get(activation, activation)\n",
        "\n",
        "        # He initialization\n",
        "        fan_in = kernel_size[0] * kernel_size[1] * in_channels\n",
        "        stddev = np.sqrt(2.0 / fan_in)\n",
        "\n",
        "        # Kernel shape: (out_channels, in_channels, height, width)\n",
        "        self.kernel = torch.randn(\n",
        "            out_channels, in_channels, kernel_size[0], kernel_size[1],\n",
        "            requires_grad=True\n",
        "        ) * stddev\n",
        "\n",
        "        if use_bias:\n",
        "            self.bias = torch.zeros(out_channels, requires_grad=True)\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Forward pass using F.conv2d\"\"\"\n",
        "        out = F.conv2d(x, self.kernel, bias=self.bias,\n",
        "                       stride=self.stride, padding=self.padding)\n",
        "        return self.activation(out)\n",
        "\n",
        "    def parameters(self):\n",
        "        if self.use_bias:\n",
        "            return [self.kernel, self.bias]\n",
        "        return [self.kernel]\n",
        "\n",
        "# Test\n",
        "conv = Conv2DLayerPrimitive(3, 16, kernel_size=3, padding=1, activation='relu')\n",
        "x = torch.randn(1, 3, 28, 28)  # 1 image, 3 channels (RGB), 28x28\n",
        "y = conv(x)\n",
        "\n",
        "print(f\"\\nConv2DLayerPrimitive(3, 16, kernel_size=3, padding=1)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Kernel shape: {conv.kernel.shape}\")\n",
        "print(f\"Parameters:   {conv.kernel.numel() + conv.bias.numel():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cz6Uihdn130p",
        "outputId": "7e8c4550-c40f-4271-a58b-613310fb6003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          COMPLETE CNN FROM PRIMITIVES\n",
            "============================================================\n",
            "\n",
            "==================================================\n",
            "        CNN FROM PRIMITIVES - SUMMARY\n",
            "==================================================\n",
            "Layer 1: Conv2DLayerPrimitive | Params: 320\n",
            "Layer 2: Conv2DLayerPrimitive | Params: 18,496\n",
            "Layer 3: DenseLayerPrimitive  | Params: 401,536\n",
            "Layer 4: DenseLayerPrimitive  | Params: 1,290\n",
            "--------------------------------------------------\n",
            "Total trainable parameters: 421,642\n",
            "\n",
            "Input shape:  torch.Size([4, 1, 28, 28])\n",
            "Output shape: torch.Size([4, 10])\n",
            "Output sum per sample: [0.9999999403953552, 1.0, 1.0, 1.0]  (should be ~1.0)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    COMPLETE CNN FROM PRIMITIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          COMPLETE CNN FROM PRIMITIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class CNNFromPrimitives:\n",
        "    \"\"\"\n",
        "    A complete CNN built using only primitive layers.\n",
        "\n",
        "    Architecture:\n",
        "        Conv(3x3) -> ReLU -> MaxPool -> Conv(3x3) -> ReLU -> MaxPool -> Flatten -> Dense -> Dense\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        self.input_shape = input_shape\n",
        "        in_channels = input_shape[0]  # PyTorch: (C, H, W)\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = Conv2DLayerPrimitive(in_channels, 32, kernel_size=3, padding=1, activation='relu')\n",
        "        self.conv2 = Conv2DLayerPrimitive(32, 64, kernel_size=3, padding=1, activation='relu')\n",
        "\n",
        "        # Calculate flattened size after convolutions and pooling\n",
        "        # With same padding and 2x2 pooling twice: H/4, W/4\n",
        "        h, w = input_shape[1] // 4, input_shape[2] // 4\n",
        "        flat_size = h * w * 64\n",
        "\n",
        "        # Dense layers\n",
        "        self.fc1 = DenseLayerPrimitive(flat_size, 128, activation='relu')\n",
        "        self.fc2 = DenseLayerPrimitive(128, num_classes, activation='softmax')\n",
        "\n",
        "        self.layers = [self.conv1, self.conv2, self.fc1, self.fc2]\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        # Conv block 1\n",
        "        x = self.conv1(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "\n",
        "        # Conv block 2\n",
        "        x = self.conv2(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Dense layers with dropout\n",
        "        x = self.fc1(x)\n",
        "        if training:\n",
        "            x = dropout_from_scratch(x, rate=0.5, training=True)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def parameters(self):\n",
        "        params = []\n",
        "        for layer in self.layers:\n",
        "            params.extend(layer.parameters())\n",
        "        return params\n",
        "\n",
        "    def summary(self):\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"        CNN FROM PRIMITIVES - SUMMARY\")\n",
        "        print(\"=\"*50)\n",
        "        total = 0\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            params = sum(p.numel() for p in layer.parameters())\n",
        "            total += params\n",
        "            print(f\"Layer {i+1}: {layer.__class__.__name__:20} | Params: {params:,}\")\n",
        "        print(\"-\"*50)\n",
        "        print(f\"Total trainable parameters: {total:,}\")\n",
        "\n",
        "# Create and test\n",
        "cnn = CNNFromPrimitives(input_shape=(1, 28, 28), num_classes=10)\n",
        "cnn.summary()\n",
        "\n",
        "# Test forward pass\n",
        "x = torch.randn(4, 1, 28, 28)\n",
        "y = cnn(x)\n",
        "print(f\"\\nInput shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Output sum per sample: {y.sum(dim=1).tolist()}  (should be ~1.0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ7pcu0G130p"
      },
      "source": [
        "---\n",
        "\n",
        "# Part IV: Custom nn.Module Layers\n",
        "\n",
        "## The Proper Way to Build Custom Layers\n",
        "\n",
        "PyTorch's `nn.Module` provides a clean API for custom layers with:\n",
        "- **Automatic parameter registration** with nn.Parameter\n",
        "- **forward() method** for the forward pass\n",
        "- **Proper device management** with `.to(device)`\n",
        "\n",
        "This gives you all the benefits of primitives PLUS:\n",
        "- Automatic weight tracking via `.parameters()`\n",
        "- Serialization with `state_dict()`\n",
        "- Integration with optimizers and training loops\n",
        "- Proper shape inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RoTZXGj130p",
        "outputId": "e29b1f80-6d24-4f08-fd6d-0c7da77ab6cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          CUSTOM nn.Module LAYER: BASICS\n",
            "============================================================\n",
            "\n",
            "CustomDenseLayer(16, 32, activation='relu')\n",
            "Input shape:  torch.Size([4, 16])\n",
            "Output shape: torch.Size([4, 32])\n",
            "Weight shape: torch.Size([16, 32])\n",
            "Trainable parameters: 544\n",
            "\n",
            "Model:\n",
            "CustomDenseLayer(\n",
            "  in_features=16, out_features=32, bias=True\n",
            "  (activation): ReLU()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM nn.Module LAYER: BASICS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          CUSTOM nn.Module LAYER: BASICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class CustomDenseLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Custom Dense layer demonstrating the nn.Module API.\n",
        "\n",
        "    Key points:\n",
        "    - Inherit from nn.Module\n",
        "    - Call super().__init__() in __init__\n",
        "    - Use nn.Parameter for learnable weights\n",
        "    - Implement forward() method\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, activation=None, use_bias=True):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        # nn.Parameter automatically registers weights!\n",
        "        self.weight = nn.Parameter(\n",
        "            torch.randn(in_features, out_features) * np.sqrt(2.0 / in_features)\n",
        "        )\n",
        "\n",
        "        if use_bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        # Store activation\n",
        "        self.activation = {\n",
        "            None: nn.Identity(),\n",
        "            'relu': nn.ReLU(),\n",
        "            'sigmoid': nn.Sigmoid(),\n",
        "            'tanh': nn.Tanh()\n",
        "        }.get(activation, activation)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        output = x @ self.weight\n",
        "        if self.bias is not None:\n",
        "            output = output + self.bias\n",
        "        return self.activation(output)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        \"\"\"Extra info for print(model).\"\"\"\n",
        "        return f'in_features={self.in_features}, out_features={self.out_features}, bias={self.use_bias}'\n",
        "\n",
        "# Test custom layer\n",
        "custom_dense = CustomDenseLayer(16, 32, activation='relu')\n",
        "x = torch.randn(4, 16)\n",
        "y = custom_dense(x)\n",
        "\n",
        "print(f\"\\nCustomDenseLayer(16, 32, activation='relu')\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Weight shape: {custom_dense.weight.shape}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in custom_dense.parameters())}\")\n",
        "print(f\"\\nModel:\")\n",
        "print(custom_dense)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Xlc5mqo130p",
        "outputId": "91819087-b400-4531-8850-a24e40572835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         CUSTOM LAYER: SELF-ATTENTION\n",
            "============================================================\n",
            "\n",
            "SelfAttentionLayer(embed_dim=64, num_heads=4)\n",
            "Input shape:             torch.Size([2, 10, 64])\n",
            "Output shape:            torch.Size([2, 10, 64])\n",
            "Attention weights shape: torch.Size([2, 4, 10, 10])\n",
            "Trainable parameters:    16,384\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM LAYER: SELF-ATTENTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"         CUSTOM LAYER: SELF-ATTENTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class SelfAttentionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Self-Attention layer (simplified version of Transformer attention).\n",
        "\n",
        "    Attention(Q, K, V) = softmax(Q @ K^T / sqrt(d_k)) @ V\n",
        "\n",
        "    In self-attention, Q, K, V all come from the same input.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads=1):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        # Linear projections for Q, K, V\n",
        "        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.W_o = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # Linear projections\n",
        "        Q = self.W_q(x)  # (batch, seq, embed)\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        # Now: (batch, heads, seq, head_dim)\n",
        "\n",
        "        # Attention scores: Q @ K^T / sqrt(d_k)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        # Softmax\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply attention to values\n",
        "        context = torch.matmul(attention_weights, V)  # (batch, heads, seq, head_dim)\n",
        "\n",
        "        # Reshape back\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
        "\n",
        "        # Output projection\n",
        "        output = self.W_o(context)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "# Test\n",
        "attention = SelfAttentionLayer(embed_dim=64, num_heads=4)\n",
        "x = torch.randn(2, 10, 64)  # (batch, seq_len, embed_dim)\n",
        "output, weights = attention(x)\n",
        "\n",
        "print(f\"\\nSelfAttentionLayer(embed_dim=64, num_heads=4)\")\n",
        "print(f\"Input shape:             {x.shape}\")\n",
        "print(f\"Output shape:            {output.shape}\")\n",
        "print(f\"Attention weights shape: {weights.shape}\")\n",
        "print(f\"Trainable parameters:    {sum(p.numel() for p in attention.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOwGtJYQ130p",
        "outputId": "07574034-b319-4662-8e5c-da6dddd6744e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "       CUSTOM LAYER: SPECTRAL NORMALIZATION\n",
            "============================================================\n",
            "\n",
            "SpectralNorm(32, 64)\n",
            "Input shape:  torch.Size([4, 32])\n",
            "Output shape: torch.Size([4, 64])\n",
            "\n",
            " Use case: Stabilize GAN discriminator training\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM LAYER: SPECTRAL NORMALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"       CUSTOM LAYER: SPECTRAL NORMALIZATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class SpectralNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Spectral Normalization wrapper for linear layers.\n",
        "\n",
        "    Constrains the spectral norm (largest singular value) of the weight matrix.\n",
        "    Used in GANs to stabilize training.\n",
        "\n",
        "    W_normalized = W / sigma(W)\n",
        "    where sigma(W) is the largest singular value.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, power_iterations=1):\n",
        "        super().__init__()\n",
        "        self.power_iterations = power_iterations\n",
        "\n",
        "        # Weight matrix\n",
        "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
        "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
        "\n",
        "        # Register buffers for u and v vectors (not parameters, but saved in state_dict)\n",
        "        self.register_buffer('u', torch.randn(out_features))\n",
        "        self.register_buffer('v', torch.randn(in_features))\n",
        "\n",
        "    def _power_iteration(self):\n",
        "        \"\"\"Estimate largest singular value using power iteration.\"\"\"\n",
        "        u = self.u\n",
        "        v = self.v\n",
        "\n",
        "        for _ in range(self.power_iterations):\n",
        "            # v = W^T u / ||W^T u||\n",
        "            v = F.normalize(self.weight.t() @ u, dim=0)\n",
        "            # u = W v / ||W v||\n",
        "            u = F.normalize(self.weight @ v, dim=0)\n",
        "\n",
        "        # Update buffers (detached)\n",
        "        self.u.copy_(u.detach())\n",
        "        self.v.copy_(v.detach())\n",
        "\n",
        "        # Spectral norm: sigma = u^T W v\n",
        "        sigma = torch.dot(u, self.weight @ v)\n",
        "        return sigma\n",
        "\n",
        "    def forward(self, x):\n",
        "        sigma = self._power_iteration()\n",
        "        weight_normalized = self.weight / sigma\n",
        "        return F.linear(x, weight_normalized, self.bias)\n",
        "\n",
        "# Test\n",
        "spectral_layer = SpectralNorm(32, 64)\n",
        "x = torch.randn(4, 32)\n",
        "y = spectral_layer(x)\n",
        "\n",
        "print(f\"\\nSpectralNorm(32, 64)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"\\n Use case: Stabilize GAN discriminator training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQJeSE01130p"
      },
      "source": [
        "---\n",
        "\n",
        "# Part V: Advanced Architectures\n",
        "\n",
        "## Building Modern Deep Learning Components\n",
        "\n",
        "Now let's build advanced architectural components used in state-of-the-art models:\n",
        "\n",
        "- **Residual Blocks** (ResNet)\n",
        "- **Squeeze-and-Excitation** (SENet)\n",
        "- **Transformer Encoder Block**\n",
        "- **Custom Normalization Layers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQuurStD130p",
        "outputId": "3cc966b1-37d5-4306-eaee-206374c14df6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           RESIDUAL BLOCK (ResNet)\n",
            "============================================================\n",
            "\n",
            "ResidualBlock(64, 64)\n",
            "Input shape:  torch.Size([2, 64, 32, 32])\n",
            "Output shape: torch.Size([2, 64, 32, 32])\n",
            "\n",
            "ResidualBlock(64, 128, stride=2)\n",
            "Output shape: torch.Size([2, 128, 16, 16])  (spatial dims halved, channels doubled)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    RESIDUAL BLOCK (ResNet Style)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           RESIDUAL BLOCK (ResNet)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual Block: y = F(x) + x\n",
        "\n",
        "    The key insight: learning residual F(x) = y - x is easier than learning y directly.\n",
        "    This enables training very deep networks (100+ layers).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Main path\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Skip connection (identity or projection)\n",
        "        self.skip = nn.Identity()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.skip = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
        "                          stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Main path\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "\n",
        "        # Skip connection + activation\n",
        "        return F.relu(out + self.skip(x))\n",
        "\n",
        "# Test\n",
        "res_block = ResidualBlock(64, 64, stride=1)\n",
        "x = torch.randn(2, 64, 32, 32)\n",
        "y = res_block(x)\n",
        "\n",
        "print(f\"\\nResidualBlock(64, 64)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "\n",
        "# With downsampling\n",
        "res_block_down = ResidualBlock(64, 128, stride=2)\n",
        "y_down = res_block_down(x)\n",
        "print(f\"\\nResidualBlock(64, 128, stride=2)\")\n",
        "print(f\"Output shape: {y_down.shape}  (spatial dims halved, channels doubled)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IB1DJ8LX130p",
        "outputId": "cf6bdd08-c6e0-4669-9c8c-e9d22f54f245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         SQUEEZE-AND-EXCITATION BLOCK\n",
            "============================================================\n",
            "\n",
            "SqueezeExcitationBlock(channels=64, reduction_ratio=16)\n",
            "Input shape:  torch.Size([2, 64, 28, 28])\n",
            "Output shape: torch.Size([2, 64, 28, 28])\n",
            "\n",
            " SE learns which channels are important for the task\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    SQUEEZE-AND-EXCITATION BLOCK\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"         SQUEEZE-AND-EXCITATION BLOCK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class SqueezeExcitationBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Squeeze-and-Excitation (SE) Block.\n",
        "\n",
        "    Learns channel-wise attention weights:\n",
        "    1. Squeeze: Global average pooling (B,C,H,W) -> (B,C,1,1)\n",
        "    2. Excitation: FC -> ReLU -> FC -> Sigmoid\n",
        "    3. Scale: Multiply input by attention weights\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, reduction_ratio=16):\n",
        "        super().__init__()\n",
        "        reduced_channels = max(channels // reduction_ratio, 1)\n",
        "\n",
        "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
        "        self.excitation = nn.Sequential(\n",
        "            nn.Linear(channels, reduced_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(reduced_channels, channels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, _, _ = x.shape\n",
        "\n",
        "        # Squeeze: Global Average Pooling\n",
        "        squeezed = self.squeeze(x).view(batch_size, channels)\n",
        "\n",
        "        # Excitation: Learn channel importance\n",
        "        attention = self.excitation(squeezed).view(batch_size, channels, 1, 1)\n",
        "\n",
        "        # Scale: Apply attention\n",
        "        return x * attention\n",
        "\n",
        "# Test\n",
        "se_block = SqueezeExcitationBlock(channels=64, reduction_ratio=16)\n",
        "x = torch.randn(2, 64, 28, 28)\n",
        "y = se_block(x)\n",
        "\n",
        "print(f\"\\nSqueezeExcitationBlock(channels=64, reduction_ratio=16)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"\\n SE learns which channels are important for the task\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrWgtJfV130p",
        "outputId": "d3250e94-e2bd-4c9f-8bff-603d3e99e292"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          TRANSFORMER ENCODER BLOCK\n",
            "============================================================\n",
            "\n",
            "TransformerEncoderBlock(embed_dim=64, num_heads=4, ff_dim=256)\n",
            "Input shape:  torch.Size([2, 20, 64])\n",
            "Output shape: torch.Size([2, 20, 64])\n",
            "Parameters:   49,984\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    TRANSFORMER ENCODER BLOCK\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          TRANSFORMER ENCODER BLOCK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer Encoder Block.\n",
        "\n",
        "    Architecture (Pre-Norm):\n",
        "        x -> LayerNorm -> MultiHeadAttention -> + (residual) ->\n",
        "             LayerNorm -> FeedForward -> + (residual) -> output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Multi-head attention\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ff_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Pre-norm architecture (more stable)\n",
        "        # Attention block\n",
        "        normed = self.norm1(x)\n",
        "        attn_output, _ = self.attention(normed, normed, normed, attn_mask=mask)\n",
        "        x = x + self.dropout(attn_output)  # Residual connection\n",
        "\n",
        "        # Feed-forward block\n",
        "        normed = self.norm2(x)\n",
        "        ff_output = self.ffn(normed)\n",
        "        x = x + ff_output  # Residual connection\n",
        "\n",
        "        return x\n",
        "\n",
        "# Test\n",
        "transformer_block = TransformerEncoderBlock(\n",
        "    embed_dim=64,\n",
        "    num_heads=4,\n",
        "    ff_dim=256,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "x = torch.randn(2, 20, 64)  # (batch, seq_len, embed_dim)\n",
        "y = transformer_block(x)\n",
        "\n",
        "print(f\"\\nTransformerEncoderBlock(embed_dim=64, num_heads=4, ff_dim=256)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Parameters:   {sum(p.numel() for p in transformer_block.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmeC-7iT130p"
      },
      "source": [
        "---\n",
        "\n",
        "# Part VI: Custom Training Loops\n",
        "\n",
        "## Full Control Over Training\n",
        "\n",
        "While high-level training abstractions are convenient, custom training loops give you:\n",
        "\n",
        "- **Complete control** over the training process\n",
        "- **Custom metrics** and logging\n",
        "- **Complex training schemes** (GANs, reinforcement learning)\n",
        "- **Gradient manipulation** (clipping, accumulation)\n",
        "- **Multi-GPU/TPU** strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kteN0deX130p",
        "outputId": "3bd09a4e-4ffb-42aa-cd99-8b2eab4f34c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          BASIC CUSTOM TRAINING LOOP\n",
            "============================================================\n",
            "\n",
            "Custom training loop template created!\n",
            "Key components:\n",
            "  1. model.train() - enables dropout, batch norm training mode\n",
            "  2. optimizer.zero_grad() - clears old gradients\n",
            "  3. loss.backward() - computes gradients\n",
            "  4. optimizer.step() - updates weights\n",
            "  5. model.eval() + torch.no_grad() - for validation\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    BASIC CUSTOM TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          BASIC CUSTOM TRAINING LOOP\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def custom_training_loop(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    epochs,\n",
        "    learning_rate=0.001,\n",
        "    device='cpu'\n",
        "):\n",
        "    \"\"\"\n",
        "    Complete custom training loop.\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # ----- TRAINING PHASE -----\n",
        "        model.train()\n",
        "        train_loss, train_correct, train_total = 0, 0, 0\n",
        "\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(x_batch)\n",
        "            loss = loss_fn(outputs, y_batch)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track metrics\n",
        "            train_loss += loss.item() * x_batch.size(0)\n",
        "            train_correct += (outputs.argmax(dim=1) == y_batch).sum().item()\n",
        "            train_total += x_batch.size(0)\n",
        "\n",
        "        # ----- VALIDATION PHASE -----\n",
        "        model.eval()\n",
        "        val_loss, val_correct, val_total = 0, 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in val_loader:\n",
        "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "                outputs = model(x_batch)\n",
        "                loss = loss_fn(outputs, y_batch)\n",
        "\n",
        "                val_loss += loss.item() * x_batch.size(0)\n",
        "                val_correct += (outputs.argmax(dim=1) == y_batch).sum().item()\n",
        "                val_total += x_batch.size(0)\n",
        "\n",
        "        # Record history\n",
        "        history['train_loss'].append(train_loss / train_total)\n",
        "        history['train_acc'].append(train_correct / train_total)\n",
        "        history['val_loss'].append(val_loss / val_total)\n",
        "        history['val_acc'].append(val_correct / val_total)\n",
        "\n",
        "        # Print progress\n",
        "        if (epoch + 1) % max(1, epochs // 10) == 0:\n",
        "            print(f\"Epoch {epoch+1:4d}/{epochs} | \"\n",
        "                  f\"Train Loss: {history['train_loss'][-1]:.4f} | \"\n",
        "                  f\"Train Acc: {history['train_acc'][-1]:.4f} | \"\n",
        "                  f\"Val Loss: {history['val_loss'][-1]:.4f} | \"\n",
        "                  f\"Val Acc: {history['val_acc'][-1]:.4f}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "print(\"\\nCustom training loop template created!\")\n",
        "print(\"Key components:\")\n",
        "print(\"  1. model.train() - enables dropout, batch norm training mode\")\n",
        "print(\"  2. optimizer.zero_grad() - clears old gradients\")\n",
        "print(\"  3. loss.backward() - computes gradients\")\n",
        "print(\"  4. optimizer.step() - updates weights\")\n",
        "print(\"  5. model.eval() + torch.no_grad() - for validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTGa8i9A130p",
        "outputId": "36d02676-bd79-4eb7-9227-7a5856d0b90e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        TRAINING WITH GRADIENT CLIPPING\n",
            "============================================================\n",
            "Gradient Clipping Options in PyTorch:\n",
            "  - torch.nn.utils.clip_grad_value_(params, clip): Clip element-wise\n",
            "  - torch.nn.utils.clip_grad_norm_(params, max_norm): Clip by global norm\n",
            "\n",
            "Global norm is most common - maintains gradient direction!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    TRAINING WITH GRADIENT CLIPPING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        TRAINING WITH GRADIENT CLIPPING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def train_step_with_clipping(model, x, y, optimizer, loss_fn, clip_norm=1.0):\n",
        "    \"\"\"\n",
        "    Single training step with gradient clipping.\n",
        "\n",
        "    Gradient clipping prevents exploding gradients in deep networks.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(x)\n",
        "    loss = loss_fn(outputs, y)\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients by global norm\n",
        "    total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
        "\n",
        "    # Apply clipped gradients\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item(), total_norm.item()\n",
        "\n",
        "print(\"Gradient Clipping Options in PyTorch:\")\n",
        "print(\"  - torch.nn.utils.clip_grad_value_(params, clip): Clip element-wise\")\n",
        "print(\"  - torch.nn.utils.clip_grad_norm_(params, max_norm): Clip by global norm\")\n",
        "print(\"\\nGlobal norm is most common - maintains gradient direction!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWvo8bzn130q",
        "outputId": "b47e9cc1-b2a0-49ab-d23a-5a6274378622"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        LEARNING RATE SCHEDULING\n",
            "============================================================\n",
            "\n",
            "Available LR Schedulers:\n",
            "  - StepLR\n",
            "  - ExponentialLR\n",
            "  - CosineAnnealingLR\n",
            "  - ReduceLROnPlateau\n",
            "  - OneCycleLR\n",
            "\n",
            "Usage in training loop:\n",
            "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
            "  for epoch in range(epochs):\n",
            "      train_one_epoch(...)\n",
            "      scheduler.step()  # Update learning rate\n",
            "\n",
            "Linear Warmup Pattern:\n",
            "  warmup_steps = 1000\n",
            "  for step in range(warmup_steps):\n",
            "      lr = base_lr * (step / warmup_steps)\n",
            "      for param_group in optimizer.param_groups:\n",
            "          param_group['lr'] = lr\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#            LEARNING RATE SCHEDULING AND WARMUP\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        LEARNING RATE SCHEDULING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# PyTorch provides many schedulers!\n",
        "model = nn.Linear(10, 2)  # Dummy model\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Common schedulers\n",
        "schedulers = {\n",
        "    'StepLR': torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1),\n",
        "    'ExponentialLR': torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95),\n",
        "    'CosineAnnealingLR': torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50),\n",
        "    'ReduceLROnPlateau': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5),\n",
        "    'OneCycleLR': torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, total_steps=100),\n",
        "}\n",
        "\n",
        "print(\"\\nAvailable LR Schedulers:\")\n",
        "for name in schedulers:\n",
        "    print(f\"  - {name}\")\n",
        "\n",
        "print(\"\\nUsage in training loop:\")\n",
        "print(\"  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\")\n",
        "print(\"  for epoch in range(epochs):\")\n",
        "print(\"      train_one_epoch(...)\")\n",
        "print(\"      scheduler.step()  # Update learning rate\")\n",
        "\n",
        "# Warmup example\n",
        "print(\"\\nLinear Warmup Pattern:\")\n",
        "print(\"  warmup_steps = 1000\")\n",
        "print(\"  for step in range(warmup_steps):\")\n",
        "print(\"      lr = base_lr * (step / warmup_steps)\")\n",
        "print(\"      for param_group in optimizer.param_groups:\")\n",
        "print(\"          param_group['lr'] = lr\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5E8_Cmh130q"
      },
      "source": [
        "---\n",
        "\n",
        "# Part VII: Practical Demos\n",
        "\n",
        "## Putting It All Together\n",
        "\n",
        "Let's combine everything we've learned in a real example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6i9U7z-130q",
        "outputId": "5068cfc2-124c-41a4-e9c6-54879f65ec5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "     DEMO: CUSTOM RESNET FOR DIGIT CLASSIFICATION\n",
            "============================================================\n",
            "Training samples: 1437\n",
            "Test samples:     360\n",
            "Image shape:      torch.Size([1, 8, 8])\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#              DEMO: CUSTOM RESNET FOR DIGIT CLASSIFICATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"     DEMO: CUSTOM RESNET FOR DIGIT CLASSIFICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load digits dataset\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "# Preprocess\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape for CNN: (samples, 1, 8, 8)\n",
        "X = X.reshape(-1, 1, 8, 8).astype(np.float32)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to tensors\n",
        "X_train = torch.tensor(X_train)\n",
        "X_test = torch.tensor(X_test)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "print(f\"Training samples: {X_train.shape[0]}\")\n",
        "print(f\"Test samples:     {X_test.shape[0]}\")\n",
        "print(f\"Image shape:      {X_train.shape[1:]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uqoufajq130q",
        "outputId": "99bddeb7-95c4-437a-ad56-7910bf309564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MiniResNet Architecture:\n",
            "MiniResNet(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (res_block1): ResidualBlock(\n",
            "    (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (skip): Identity()\n",
            "  )\n",
            "  (res_block2): ResidualBlock(\n",
            "    (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (skip): Sequential(\n",
            "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (se_block): SqueezeExcitationBlock(\n",
            "    (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
            "    (excitation): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=8, bias=True)\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Linear(in_features=8, out_features=64, bias=True)\n",
            "      (3): Sigmoid()\n",
            "    )\n",
            "  )\n",
            "  (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n",
            "\n",
            "Total parameters: 78,418\n"
          ]
        }
      ],
      "source": [
        "# Build custom ResNet model\n",
        "class MiniResNet(nn.Module):\n",
        "    \"\"\"Mini ResNet with custom residual blocks.\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initial convolution\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Residual blocks\n",
        "        self.res_block1 = ResidualBlock(32, 32)\n",
        "        self.res_block2 = ResidualBlock(32, 64, stride=2)\n",
        "\n",
        "        # SE block for channel attention\n",
        "        self.se_block = SqueezeExcitationBlock(64, reduction_ratio=8)\n",
        "\n",
        "        # Classification head\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial conv\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "        # Residual blocks\n",
        "        x = self.res_block1(x)\n",
        "        x = self.res_block2(x)\n",
        "\n",
        "        # SE attention\n",
        "        x = self.se_block(x)\n",
        "\n",
        "        # Classification head\n",
        "        x = self.global_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Create model\n",
        "torch.manual_seed(42)\n",
        "resnet_model = MiniResNet(num_classes=10).to(device)\n",
        "\n",
        "print(\"\\nMiniResNet Architecture:\")\n",
        "print(resnet_model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in resnet_model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKewny5v130q",
        "outputId": "6cd894f1-b283-4e1e-ecdc-21415ef00462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training MiniResNet...\n",
            "\n",
            "Epoch  5/30 | Train Loss: 0.0729 | Train Acc: 0.9951 | Val Acc: 0.9750\n",
            "Epoch 10/30 | Train Loss: 0.0267 | Train Acc: 0.9972 | Val Acc: 0.9833\n",
            "Epoch 15/30 | Train Loss: 0.0096 | Train Acc: 1.0000 | Val Acc: 0.9944\n",
            "Epoch 20/30 | Train Loss: 0.0053 | Train Acc: 1.0000 | Val Acc: 0.9944\n",
            "Epoch 25/30 | Train Loss: 0.0045 | Train Acc: 1.0000 | Val Acc: 0.9944\n",
            "Epoch 30/30 | Train Loss: 0.0051 | Train Acc: 1.0000 | Val Acc: 0.9917\n",
            "\n",
            "Final Test Accuracy: 99.17%\n"
          ]
        }
      ],
      "source": [
        "# Create data loaders\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Training setup\n",
        "optimizer = torch.optim.Adam(resnet_model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "print(\"\\nTraining MiniResNet...\\n\")\n",
        "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "for epoch in range(30):\n",
        "    # Training\n",
        "    resnet_model.train()\n",
        "    train_loss, train_correct, train_total = 0, 0, 0\n",
        "\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = resnet_model(x_batch)\n",
        "        loss = loss_fn(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * x_batch.size(0)\n",
        "        train_correct += (outputs.argmax(dim=1) == y_batch).sum().item()\n",
        "        train_total += x_batch.size(0)\n",
        "\n",
        "    # Validation\n",
        "    resnet_model.eval()\n",
        "    val_loss, val_correct, val_total = 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in test_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            outputs = resnet_model(x_batch)\n",
        "            loss = loss_fn(outputs, y_batch)\n",
        "\n",
        "            val_loss += loss.item() * x_batch.size(0)\n",
        "            val_correct += (outputs.argmax(dim=1) == y_batch).sum().item()\n",
        "            val_total += x_batch.size(0)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Record\n",
        "    history['train_loss'].append(train_loss / train_total)\n",
        "    history['train_acc'].append(train_correct / train_total)\n",
        "    history['val_loss'].append(val_loss / val_total)\n",
        "    history['val_acc'].append(val_correct / val_total)\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1:2d}/30 | \"\n",
        "              f\"Train Loss: {history['train_loss'][-1]:.4f} | \"\n",
        "              f\"Train Acc: {history['train_acc'][-1]:.4f} | \"\n",
        "              f\"Val Acc: {history['val_acc'][-1]:.4f}\")\n",
        "\n",
        "print(f\"\\nFinal Test Accuracy: {history['val_acc'][-1]*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        },
        "id": "8iNpgt2m130q",
        "outputId": "803e24ee-1873-4a5f-f14c-de089cc841b4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x600 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    VISUALIZE TRAINING RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "# Plot training history\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax1.plot(history['train_loss'], 'b-', label='Train Loss')\n",
        "ax1.plot(history['val_loss'], 'r-', label='Val Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('MiniResNet Training Loss', fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.plot(history['train_acc'], 'b-', label='Train Acc')\n",
        "ax2.plot(history['val_acc'], 'r-', label='Val Acc')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('MiniResNet Training Accuracy', fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize predictions\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "fig.suptitle('MiniResNet Predictions', fontsize=14, fontweight='bold')\n",
        "\n",
        "resnet_model.eval()\n",
        "with torch.no_grad():\n",
        "    X_sample = X_test[:10].to(device)\n",
        "    predictions = resnet_model(X_sample)\n",
        "    probs = F.softmax(predictions, dim=1)\n",
        "    pred_classes = predictions.argmax(dim=1).cpu()\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    img = X_test[i, 0].numpy()\n",
        "    true_label = y_test[i].item()\n",
        "    pred_label = pred_classes[i].item()\n",
        "    confidence = probs[i, pred_label].item() * 100\n",
        "\n",
        "    ax.imshow(img, cmap='gray')\n",
        "    color = 'green' if true_label == pred_label else 'red'\n",
        "    ax.set_title(f'True: {true_label}, Pred: {pred_label}\\n({confidence:.1f}%)',\n",
        "                 color=color, fontsize=10)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJz3WzGk130q",
        "outputId": "8336b9e6-0592-4d22-ba0a-9dc6074cae9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "                 ADVANCED PYTORCH CHEAT SHEET\n",
            "======================================================================\n",
            "\n",
            "AUTOGRAD PATTERNS\n",
            "-----------------\n",
            "# Basic gradient\n",
            "y = model(x)\n",
            "y.backward()\n",
            "grads = [p.grad for p in model.parameters()]\n",
            "\n",
            "# Higher-order derivatives\n",
            "dy_dx = torch.autograd.grad(y, x, create_graph=True)[0]\n",
            "d2y_dx2 = torch.autograd.grad(dy_dx, x)[0]\n",
            "\n",
            "# Jacobian and Hessian\n",
            "jacobian = torch.autograd.functional.jacobian(f, x)\n",
            "hessian = torch.autograd.functional.hessian(f, x)\n",
            "\n",
            "# Custom autograd function\n",
            "class CustomOp(torch.autograd.Function):\n",
            "    @staticmethod\n",
            "    def forward(ctx, x):\n",
            "        ctx.save_for_backward(x)\n",
            "        return forward_result\n",
            "    @staticmethod\n",
            "    def backward(ctx, grad_output):\n",
            "        x, = ctx.saved_tensors\n",
            "        return custom_gradient\n",
            "\n",
            "CUSTOM nn.Module LAYERS\n",
            "-----------------------\n",
            "class CustomLayer(nn.Module):\n",
            "    def __init__(self, in_features, out_features):\n",
            "        super().__init__()\n",
            "        self.weight = nn.Parameter(torch.randn(in_features, out_features))\n",
            "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
            "\n",
            "    def forward(self, x):\n",
            "        return x @ self.weight + self.bias\n",
            "\n",
            "CUSTOM TRAINING\n",
            "---------------\n",
            "# Training loop\n",
            "model.train()\n",
            "for x, y in train_loader:\n",
            "    optimizer.zero_grad()\n",
            "    loss = loss_fn(model(x), y)\n",
            "    loss.backward()\n",
            "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
            "    optimizer.step()\n",
            "scheduler.step()\n",
            "\n",
            "# Validation loop\n",
            "model.eval()\n",
            "with torch.no_grad():\n",
            "    for x, y in val_loader:\n",
            "        outputs = model(x)\n",
            "\n",
            "GRADIENT MANIPULATION\n",
            "---------------------\n",
            "# Clip by global norm\n",
            "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
            "\n",
            "# Gradient accumulation\n",
            "for i, (x, y) in enumerate(loader):\n",
            "    loss = loss_fn(model(x), y) / accumulation_steps\n",
            "    loss.backward()\n",
            "    if (i + 1) % accumulation_steps == 0:\n",
            "        optimizer.step()\n",
            "        optimizer.zero_grad()\n",
            "\n",
            "# Gradient hooks\n",
            "handle = tensor.register_hook(lambda grad: grad.clamp(-1, 1))\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                         ADVANCED CHEAT SHEET\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"                 ADVANCED PYTORCH CHEAT SHEET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "cheat_sheet = \"\"\"\n",
        "AUTOGRAD PATTERNS\n",
        "-----------------\n",
        "# Basic gradient\n",
        "y = model(x)\n",
        "y.backward()\n",
        "grads = [p.grad for p in model.parameters()]\n",
        "\n",
        "# Higher-order derivatives\n",
        "dy_dx = torch.autograd.grad(y, x, create_graph=True)[0]\n",
        "d2y_dx2 = torch.autograd.grad(dy_dx, x)[0]\n",
        "\n",
        "# Jacobian and Hessian\n",
        "jacobian = torch.autograd.functional.jacobian(f, x)\n",
        "hessian = torch.autograd.functional.hessian(f, x)\n",
        "\n",
        "# Custom autograd function\n",
        "class CustomOp(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x):\n",
        "        ctx.save_for_backward(x)\n",
        "        return forward_result\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        x, = ctx.saved_tensors\n",
        "        return custom_gradient\n",
        "\n",
        "CUSTOM nn.Module LAYERS\n",
        "-----------------------\n",
        "class CustomLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(in_features, out_features))\n",
        "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x @ self.weight + self.bias\n",
        "\n",
        "CUSTOM TRAINING\n",
        "---------------\n",
        "# Training loop\n",
        "model.train()\n",
        "for x, y in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    loss = loss_fn(model(x), y)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "scheduler.step()\n",
        "\n",
        "# Validation loop\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "        outputs = model(x)\n",
        "\n",
        "GRADIENT MANIPULATION\n",
        "---------------------\n",
        "# Clip by global norm\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "# Gradient accumulation\n",
        "for i, (x, y) in enumerate(loader):\n",
        "    loss = loss_fn(model(x), y) / accumulation_steps\n",
        "    loss.backward()\n",
        "    if (i + 1) % accumulation_steps == 0:\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "# Gradient hooks\n",
        "handle = tensor.register_hook(lambda grad: grad.clamp(-1, 1))\n",
        "\"\"\"\n",
        "print(cheat_sheet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jguqJC8p130q"
      },
      "source": [
        "---\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "## Your Advanced PyTorch Journey\n",
        "\n",
        "Congratulations! You've mastered advanced PyTorch techniques.\n",
        "\n",
        "### What You Learned\n",
        "\n",
        "| Part | Topic | Key Takeaway |\n",
        "|------|-------|-------------|\n",
        "| I | Advanced Autograd | Nested autograd, Jacobians, custom functions |\n",
        "| II | Building Ops | Conv, pooling, normalization from scratch |\n",
        "| III | Primitive Layers | Dense, Conv2D with only basic tensors |\n",
        "| IV | Custom nn.Module | Proper subclassing with nn.Parameter |\n",
        "| V | Advanced Architectures | ResNet, SE-Net, Transformer blocks |\n",
        "| VI | Custom Training | Full control with manual training loops |\n",
        "| VII | Practical Demos | Real-world model combining everything |\n",
        "\n",
        "### When to Use What\n",
        "\n",
        "| Approach | Use When |\n",
        "|----------|----------|\n",
        "| Standard `nn.Module` | Most deep learning tasks |\n",
        "| Custom `autograd.Function` | Non-differentiable ops, custom gradients |\n",
        "| Custom training loop | GANs, RL, complex multi-model training |\n",
        "| Gradient hooks | Debugging, visualization, per-layer manipulation |\n",
        "| Primitive layers | Learning, debugging, maximum control |\n",
        "\n",
        "### The Complete Learning Path\n",
        "\n",
        "1. **NumPy from Scratch** - Understand the math deeply\n",
        "2. **PyTorch Part 1** - Fundamentals and high-level API\n",
        "3. **PyTorch Part 2** - Advanced custom components (This notebook!)\n",
        "4. **TensorFlow/Keras** - Alternative framework comparison\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- **Vision Transformers (ViT)** - Transformers for images\n",
        "- **Diffusion Models** - State-of-the-art generative AI\n",
        "- **Distributed Training** - Multi-GPU with DDP\n",
        "- **Model Optimization** - TorchScript, quantization, pruning\n",
        "- **PyTorch Lightning** - Even cleaner training code\n",
        "\n",
        "---\n",
        "\n",
        "*\"The more you understand the primitives, the better you can innovate.\"*\n",
        "\n",
        "**Happy Deep Learning!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}