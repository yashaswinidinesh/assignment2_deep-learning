{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEkrSo01WcC7"
      },
      "source": [
        "# TensorFlow & Keras Advanced: Custom Layers and Deep Architectures\n",
        "\n",
        "## Part 2: Building Complex Operations from Scratch\n",
        "\n",
        "---\n",
        "\n",
        "In Part 1, we learned TensorFlow fundamentals, GradientTape basics, and the high-level Keras API. Now we go deeper!\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "| Part | Topic | Key Concepts |\n",
        "|------|-------|-------------|\n",
        "| **I** | Advanced GradientTape | Nested tapes, Jacobians, custom gradients |\n",
        "| **II** | Building Ops from Scratch | Convolution, pooling, normalization by hand |\n",
        "| **III** | Custom Layers (Primitives) | Build layers using only tf.Variable |\n",
        "| **IV** | Custom Keras Layers | Proper subclassing with `build()` and `call()` |\n",
        "| **V** | Advanced Architectures | Residual blocks, attention, custom normalizations |\n",
        "| **VI** | Custom Training Loops | Full control over training with GradientTape |\n",
        "| **VII** | Practical Demos | Real-world examples with custom components |\n",
        "\n",
        "---\n",
        "\n",
        "*\"To understand the framework, build it from scratch.\"*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQaXbL0qWcC7",
        "outputId": "96221497-595b-4b4f-ec7d-74875120df80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.19.0\n",
            "Keras Version:      3.10.0\n",
            "GPU Available:      False\n",
            "\n",
            "Ready for Advanced TensorFlow & Keras!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                           SETUP & IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers, Model, Sequential\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Optional, Callable, Union\n",
        "\n",
        "# Beautiful plots\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Check versions\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(f\"Keras Version:      {keras.__version__}\")\n",
        "print(f\"GPU Available:      {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
        "\n",
        "# GPU memory growth\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "# Reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"\\nReady for Advanced TensorFlow & Keras!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tItbO-4WcC8"
      },
      "source": [
        "---\n",
        "\n",
        "# Part I: Advanced GradientTape Patterns\n",
        "\n",
        "## Beyond Basic Gradient Computation\n",
        "\n",
        "In Part 1, we used GradientTape for simple gradients. Now we'll explore:\n",
        "\n",
        "- **Nested tapes** for higher-order derivatives\n",
        "- **Jacobian and Hessian** computation\n",
        "- **Custom gradients** for non-differentiable operations\n",
        "- **Gradient clipping** and manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9-VhzBFWcC8",
        "outputId": "cd3d0925-0295-484d-812c-227251259291"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "       NESTED TAPES: HIGHER-ORDER DERIVATIVES\n",
            "============================================================\n",
            "\n",
            "f(x) = x^4, evaluated at x = 2.0\n",
            "\n",
            "f(x)    = 16.0         (expected: 16)\n",
            "f'(x)   = 32.0         (expected: 32 = 4*8)\n",
            "f''(x)  = 48.0         (expected: 48 = 12*4)\n",
            "f'''(x) = 48.0         (expected: 48 = 24*2)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    NESTED GRADIENTTAPES: HIGHER-ORDER DERIVATIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"       NESTED TAPES: HIGHER-ORDER DERIVATIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Example: Compute first, second, and third derivatives\n",
        "# f(x) = x^4\n",
        "# f'(x) = 4x^3\n",
        "# f''(x) = 12x^2\n",
        "# f'''(x) = 24x\n",
        "\n",
        "x = tf.Variable(2.0)\n",
        "\n",
        "with tf.GradientTape() as tape3:\n",
        "    with tf.GradientTape() as tape2:\n",
        "        with tf.GradientTape() as tape1:\n",
        "            y = x ** 4\n",
        "        dy_dx = tape1.gradient(y, x)      # First derivative: 4x^3\n",
        "    d2y_dx2 = tape2.gradient(dy_dx, x)    # Second derivative: 12x^2\n",
        "d3y_dx3 = tape3.gradient(d2y_dx2, x)      # Third derivative: 24x\n",
        "\n",
        "print(f\"\\nf(x) = x^4, evaluated at x = {x.numpy()}\")\n",
        "print(f\"\")\n",
        "print(f\"f(x)    = {y.numpy():.1f}         (expected: 16)\")\n",
        "print(f\"f'(x)   = {dy_dx.numpy():.1f}         (expected: 32 = 4*8)\")\n",
        "print(f\"f''(x)  = {d2y_dx2.numpy():.1f}         (expected: 48 = 12*4)\")\n",
        "print(f\"f'''(x) = {d3y_dx3.numpy():.1f}         (expected: 48 = 24*2)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdSJanpUWcC8",
        "outputId": "f480db51-d297-47a0-ed2c-4c02ed1f80b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "              JACOBIAN COMPUTATION\n",
            "============================================================\n",
            "\n",
            "x = [1. 2. 3.]\n",
            "y = f(x) = [x1^2, x1*x2, sin(x3)] = [1.      2.      0.14112]\n",
            "\n",
            "Jacobian (3x3):\n",
            "[[ 2.         0.         0.       ]\n",
            " [ 2.         1.         0.       ]\n",
            " [ 0.         0.        -0.9899925]]\n",
            "\n",
            "Expected Jacobian:\n",
            "  [2*x1,   0,      0   ]   = [2,   0,     0     ]\n",
            "  [x2,     x1,     0   ]   = [2,   1,     0     ]\n",
            "  [0,      0,  cos(x3) ]   = [0,   0,  -0.9900]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    JACOBIAN COMPUTATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"              JACOBIAN COMPUTATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The Jacobian is the matrix of all first-order partial derivatives\n",
        "# For f: R^n -> R^m, the Jacobian J is m x n where J[i,j] = df_i/dx_j\n",
        "\n",
        "x = tf.Variable([1.0, 2.0, 3.0])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    # Vector function: f(x) = [x1^2, x1*x2, sin(x3)]\n",
        "    y = tf.stack([\n",
        "        x[0] ** 2,\n",
        "        x[0] * x[1],\n",
        "        tf.sin(x[2])\n",
        "    ])\n",
        "\n",
        "# Compute full Jacobian\n",
        "jacobian = tape.jacobian(y, x)\n",
        "\n",
        "print(f\"\\nx = {x.numpy()}\")\n",
        "print(f\"y = f(x) = [x1^2, x1*x2, sin(x3)] = {y.numpy()}\")\n",
        "print(f\"\\nJacobian (3x3):\")\n",
        "print(f\"{jacobian.numpy()}\")\n",
        "\n",
        "print(f\"\\nExpected Jacobian:\")\n",
        "print(f\"  [2*x1,   0,      0   ]   = [2,   0,     0     ]\")\n",
        "print(f\"  [x2,     x1,     0   ]   = [2,   1,     0     ]\")\n",
        "print(f\"  [0,      0,  cos(x3) ]   = [0,   0,  {np.cos(3):.4f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_a3dkRlWcC8",
        "outputId": "c48e547b-7f8f-4a4f-c5b0-8c47707d5848"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "              HESSIAN COMPUTATION\n",
            "============================================================\n",
            "\n",
            "f(x, y) = x^2*y + y^3, at (x, y) = (1.0, 2.0)\n",
            "f = 10.0\n",
            "\n",
            "Gradient: [ 4. 13.]\n",
            "  Expected: [2xy, x^2 + 3y^2] = [4, 13]\n",
            "\n",
            "Hessian:\n",
            "[[ 4.  2.]\n",
            " [ 2. 12.]]\n",
            "  Expected:\n",
            "  [2y,  2x ]   = [4, 2]\n",
            "  [2x,  6y ]   = [2, 12]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    HESSIAN COMPUTATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"              HESSIAN COMPUTATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The Hessian is the matrix of second-order partial derivatives\n",
        "# H[i,j] = d^2f / (dx_i dx_j)\n",
        "\n",
        "x = tf.Variable([1.0, 2.0])\n",
        "\n",
        "with tf.GradientTape() as tape2:\n",
        "    with tf.GradientTape() as tape1:\n",
        "        # Scalar function: f(x, y) = x^2*y + y^3\n",
        "        f = x[0]**2 * x[1] + x[1]**3\n",
        "    grad = tape1.gradient(f, x)  # [2xy, x^2 + 3y^2]\n",
        "hessian = tape2.jacobian(grad, x)\n",
        "\n",
        "print(f\"\\nf(x, y) = x^2*y + y^3, at (x, y) = ({x[0].numpy()}, {x[1].numpy()})\")\n",
        "print(f\"f = {f.numpy()}\")\n",
        "print(f\"\\nGradient: {grad.numpy()}\")\n",
        "print(f\"  Expected: [2xy, x^2 + 3y^2] = [4, 13]\")\n",
        "print(f\"\\nHessian:\")\n",
        "print(f\"{hessian.numpy()}\")\n",
        "print(f\"  Expected:\")\n",
        "print(f\"  [2y,  2x ]   = [4, 2]\")\n",
        "print(f\"  [2x,  6y ]   = [2, 12]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srxJFP_KWcC9",
        "outputId": "8cad8236-ce45-4dc6-d4d7-07f0e066dc5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "              CUSTOM GRADIENTS\n",
            "============================================================\n",
            "\n",
            "Input: [3. 4.]\n",
            "Gradient (clipped to norm 1.0): [0.70710677 0.70710677]\n",
            "Gradient norm: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM GRADIENTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"              CUSTOM GRADIENTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Sometimes you need to define custom gradients:\n",
        "# - For non-differentiable operations (like argmax)\n",
        "# - For numerical stability\n",
        "# - For custom backward passes (like straight-through estimators)\n",
        "\n",
        "@tf.custom_gradient\n",
        "def clip_gradient_norm(x, clip_value=1.0):\n",
        "    \"\"\"\n",
        "    Forward: identity function\n",
        "    Backward: clip gradient norm\n",
        "    \"\"\"\n",
        "    def grad(dy):\n",
        "        # Clip the incoming gradient\n",
        "        norm = tf.norm(dy)\n",
        "        clipped = tf.cond(\n",
        "            norm > clip_value,\n",
        "            lambda: dy * clip_value / norm,\n",
        "            lambda: dy\n",
        "        )\n",
        "        return clipped  # Only return gradient for 'x'\n",
        "    return x, grad\n",
        "\n",
        "# Test custom gradient\n",
        "x = tf.Variable([3.0, 4.0])  # Gradient will have norm 5 (3-4-5 triangle)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y = clip_gradient_norm(x, clip_value=1.0)\n",
        "    loss = tf.reduce_sum(y)  # Gradient would be [1, 1] but we pass [3, 4]\n",
        "\n",
        "# Manually set upstream gradient to [3, 4]\n",
        "grad = tape.gradient(loss, x)\n",
        "print(f\"\\nInput: {x.numpy()}\")\n",
        "print(f\"Gradient (clipped to norm 1.0): {grad.numpy()}\")\n",
        "print(f\"Gradient norm: {tf.norm(grad).numpy():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl8-8EpEWcC9",
        "outputId": "8ff8ee77-0fd6-4e2e-c799-1fe46e4d2ea2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         STRAIGHT-THROUGH ESTIMATOR\n",
            "============================================================\n",
            "\n",
            "Input:   [0.3 0.7 1.2 2.5]\n",
            "Rounded: [0. 1. 1. 2.]\n",
            "Gradient (straight-through): [0. 2. 2. 4.]\n",
            "\n",
            " Note: Round is non-differentiable, but we can still train!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    STRAIGHT-THROUGH ESTIMATOR\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"         STRAIGHT-THROUGH ESTIMATOR\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The straight-through estimator is used for:\n",
        "# - Binary/discrete operations that are non-differentiable\n",
        "# - Quantization in neural networks\n",
        "\n",
        "@tf.custom_gradient\n",
        "def straight_through_round(x):\n",
        "    \"\"\"\n",
        "    Forward: round to nearest integer\n",
        "    Backward: pass gradient through unchanged (identity)\n",
        "    \"\"\"\n",
        "    def grad(dy):\n",
        "        return dy  # Straight-through: gradient = identity\n",
        "    return tf.round(x), grad\n",
        "\n",
        "@tf.custom_gradient\n",
        "def straight_through_sign(x):\n",
        "    \"\"\"\n",
        "    Forward: sign function (-1, 0, or 1)\n",
        "    Backward: gradient of hard tanh (1 if |x| <= 1, else 0)\n",
        "    \"\"\"\n",
        "    def grad(dy):\n",
        "        # Gradient is 1 where |x| <= 1, 0 elsewhere\n",
        "        return dy * tf.cast(tf.abs(x) <= 1, dy.dtype)\n",
        "    return tf.sign(x), grad\n",
        "\n",
        "# Test\n",
        "x = tf.Variable([0.3, 0.7, 1.2, 2.5])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y = straight_through_round(x)\n",
        "    loss = tf.reduce_sum(y ** 2)\n",
        "\n",
        "grad = tape.gradient(loss, x)\n",
        "\n",
        "print(f\"\\nInput:   {x.numpy()}\")\n",
        "print(f\"Rounded: {y.numpy()}\")\n",
        "print(f\"Gradient (straight-through): {grad.numpy()}\")\n",
        "print(f\"\\n Note: Round is non-differentiable, but we can still train!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Uwxn2-LWcC9",
        "outputId": "d0fb7051-0b93-4fe8-db64-6a91dea3a36e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "            GRADIENT ACCUMULATION\n",
            "============================================================\n",
            "\n",
            "Gradient Accumulation Pattern:\n",
            "  1. Compute gradients for mini-batch\n",
            "  2. Accumulate (sum or average) over N steps\n",
            "  3. Apply accumulated gradients once\n",
            "  4. Effective batch = mini_batch * N\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    GRADIENT ACCUMULATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"            GRADIENT ACCUMULATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Gradient accumulation is useful when:\n",
        "# - Batch size is too large for GPU memory\n",
        "# - You want effective larger batch sizes\n",
        "\n",
        "def train_with_accumulation(model, data, labels, batch_size, accumulation_steps, optimizer):\n",
        "    \"\"\"\n",
        "    Train with gradient accumulation.\n",
        "    Effective batch size = batch_size * accumulation_steps\n",
        "    \"\"\"\n",
        "    n_samples = len(data)\n",
        "    accumulated_gradients = [tf.zeros_like(v) for v in model.trainable_variables]\n",
        "\n",
        "    for step in range(accumulation_steps):\n",
        "        # Get mini-batch\n",
        "        start = (step * batch_size) % n_samples\n",
        "        end = start + batch_size\n",
        "        x_batch = data[start:end]\n",
        "        y_batch = labels[start:end]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(x_batch, training=True)\n",
        "            loss = tf.reduce_mean(keras.losses.mse(y_batch, predictions))\n",
        "\n",
        "        # Compute gradients\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "        # Accumulate (average over steps)\n",
        "        accumulated_gradients = [\n",
        "            acc + grad / accumulation_steps\n",
        "            for acc, grad in zip(accumulated_gradients, gradients)\n",
        "        ]\n",
        "\n",
        "    # Apply accumulated gradients\n",
        "    optimizer.apply_gradients(zip(accumulated_gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "print(\"\\nGradient Accumulation Pattern:\")\n",
        "print(\"  1. Compute gradients for mini-batch\")\n",
        "print(\"  2. Accumulate (sum or average) over N steps\")\n",
        "print(\"  3. Apply accumulated gradients once\")\n",
        "print(\"  4. Effective batch = mini_batch * N\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WCMnLwcWcC9"
      },
      "source": [
        "---\n",
        "\n",
        "# Part II: Building Operations from Scratch\n",
        "\n",
        "## Understanding Neural Network Primitives\n",
        "\n",
        "Before using Keras layers, let's understand what they do by building them ourselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY-nVGvvWcC-",
        "outputId": "3ae5f561-d30f-43e7-c170-327bd170df29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           CONVOLUTION FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape:  (1, 5, 5, 1)\n",
            "Kernel shape: (3, 3, 1, 2)\n",
            "Output shape: (1, 3, 3, 2)\n",
            "Matches tf.nn.conv2d: True\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CONVOLUTION FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           CONVOLUTION FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def conv2d_naive(input_tensor, kernel, stride=1, padding='VALID'):\n",
        "    \"\"\"\n",
        "    Naive 2D convolution implementation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_tensor : tensor (batch, height, width, in_channels)\n",
        "    kernel : tensor (kernel_h, kernel_w, in_channels, out_channels)\n",
        "    stride : int\n",
        "    padding : 'VALID' or 'SAME'\n",
        "    \"\"\"\n",
        "    batch_size = tf.shape(input_tensor)[0]\n",
        "    in_h, in_w = input_tensor.shape[1], input_tensor.shape[2]\n",
        "    k_h, k_w = kernel.shape[0], kernel.shape[1]\n",
        "    out_channels = kernel.shape[3]\n",
        "\n",
        "    if padding == 'SAME':\n",
        "        pad_h = k_h // 2\n",
        "        pad_w = k_w // 2\n",
        "        input_tensor = tf.pad(input_tensor,\n",
        "                              [[0, 0], [pad_h, pad_h], [pad_w, pad_w], [0, 0]])\n",
        "        in_h += 2 * pad_h\n",
        "        in_w += 2 * pad_w\n",
        "\n",
        "    out_h = (in_h - k_h) // stride + 1\n",
        "    out_w = (in_w - k_w) // stride + 1\n",
        "\n",
        "    output = tf.TensorArray(dtype=tf.float32, size=out_h * out_w)\n",
        "    idx = 0\n",
        "\n",
        "    for i in range(out_h):\n",
        "        for j in range(out_w):\n",
        "            # Extract patch\n",
        "            h_start = i * stride\n",
        "            w_start = j * stride\n",
        "            patch = input_tensor[:, h_start:h_start+k_h, w_start:w_start+k_w, :]\n",
        "\n",
        "            # Convolve: sum over (h, w, in_channels), keep out_channels\n",
        "            # patch: (batch, k_h, k_w, in_c)\n",
        "            # kernel: (k_h, k_w, in_c, out_c)\n",
        "            conv = tf.einsum('bhwi,hwio->bo', patch, kernel)\n",
        "            output = output.write(idx, conv)\n",
        "            idx += 1\n",
        "\n",
        "    output = output.stack()  # (out_h*out_w, batch, out_c)\n",
        "    output = tf.transpose(output, [1, 0, 2])  # (batch, out_h*out_w, out_c)\n",
        "    output = tf.reshape(output, [batch_size, out_h, out_w, out_channels])\n",
        "\n",
        "    return output\n",
        "\n",
        "# Test our implementation\n",
        "x = tf.random.normal((1, 5, 5, 1))  # 1 image, 5x5, 1 channel\n",
        "kernel = tf.random.normal((3, 3, 1, 2))  # 3x3 kernel, 1->2 channels\n",
        "\n",
        "our_output = conv2d_naive(x, kernel, stride=1, padding='VALID')\n",
        "tf_output = tf.nn.conv2d(x, kernel, strides=1, padding='VALID')\n",
        "\n",
        "print(f\"\\nInput shape:  {x.shape}\")\n",
        "print(f\"Kernel shape: {kernel.shape}\")\n",
        "print(f\"Output shape: {our_output.shape}\")\n",
        "print(f\"Matches tf.nn.conv2d: {tf.reduce_all(tf.abs(our_output - tf_output) < 1e-5).numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltjjl1lJWcC-",
        "outputId": "f6ea1958-3571-4b00-9a92-f247e32f6004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           MAX POOLING FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape: (1, 4, 4, 2)\n",
            "Input (channel 0):\n",
            "[[ 1.  3.  5.  7.]\n",
            " [ 9. 11. 13. 15.]\n",
            " [17. 19. 21. 23.]\n",
            " [25. 27. 29. 31.]]\n",
            "\n",
            "Output shape: (1, 2, 2, 2)\n",
            "Output (channel 0):\n",
            "[[11. 15.]\n",
            " [27. 31.]]\n",
            "\n",
            "Matches tf.nn.max_pool2d: True\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    MAX POOLING FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           MAX POOLING FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def max_pool2d_naive(input_tensor, pool_size=2, stride=2):\n",
        "    \"\"\"\n",
        "    Naive max pooling implementation.\n",
        "\n",
        "    For each pool_size x pool_size window, take the maximum.\n",
        "    \"\"\"\n",
        "    batch_size = tf.shape(input_tensor)[0]\n",
        "    in_h, in_w, channels = input_tensor.shape[1:]\n",
        "\n",
        "    out_h = (in_h - pool_size) // stride + 1\n",
        "    out_w = (in_w - pool_size) // stride + 1\n",
        "\n",
        "    outputs = []\n",
        "\n",
        "    for i in range(out_h):\n",
        "        row = []\n",
        "        for j in range(out_w):\n",
        "            h_start = i * stride\n",
        "            w_start = j * stride\n",
        "            # Extract window\n",
        "            window = input_tensor[:, h_start:h_start+pool_size,\n",
        "                                  w_start:w_start+pool_size, :]\n",
        "            # Max over spatial dimensions\n",
        "            pooled = tf.reduce_max(window, axis=[1, 2])\n",
        "            row.append(pooled)\n",
        "        outputs.append(tf.stack(row, axis=1))\n",
        "\n",
        "    return tf.stack(outputs, axis=1)\n",
        "\n",
        "# Test\n",
        "x = tf.constant([[[[1., 2.], [3., 4.], [5., 6.], [7., 8.]],\n",
        "                  [[9., 10.], [11., 12.], [13., 14.], [15., 16.]],\n",
        "                  [[17., 18.], [19., 20.], [21., 22.], [23., 24.]],\n",
        "                  [[25., 26.], [27., 28.], [29., 30.], [31., 32.]]]])\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"Input (channel 0):\")\n",
        "print(x[0, :, :, 0].numpy())\n",
        "\n",
        "our_pool = max_pool2d_naive(x, pool_size=2, stride=2)\n",
        "tf_pool = tf.nn.max_pool2d(x, ksize=2, strides=2, padding='VALID')\n",
        "\n",
        "print(f\"\\nOutput shape: {our_pool.shape}\")\n",
        "print(f\"Output (channel 0):\")\n",
        "print(our_pool[0, :, :, 0].numpy())\n",
        "print(f\"\\nMatches tf.nn.max_pool2d: {tf.reduce_all(our_pool == tf_pool).numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeZuo5ITWcC-",
        "outputId": "b9f6ac08-560e-4781-e076-d4d65d0c2469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        BATCH NORMALIZATION FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape: (8, 4)\n",
            "Input mean per feature: [ 0.32574826 -0.22160122  0.37827352  0.3575499 ]\n",
            "Input std per feature:  [0.8102391  0.74234474 1.061351   1.0821929 ]\n",
            "\n",
            "Output (training) mean: [-2.9802322e-08  4.6566129e-08  7.4505806e-09 -2.2351742e-08]\n",
            "Output (training) std:  [0.9999923 0.9999908 0.9999955 0.9999958]\n",
            "\n",
            " After BatchNorm, each feature has ~0 mean and ~1 std!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    BATCH NORMALIZATION FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        BATCH NORMALIZATION FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class BatchNormFromScratch:\n",
        "    \"\"\"\n",
        "    Batch Normalization implemented from scratch.\n",
        "\n",
        "    During training:\n",
        "        x_norm = (x - batch_mean) / sqrt(batch_var + epsilon)\n",
        "        y = gamma * x_norm + beta\n",
        "\n",
        "    During inference:\n",
        "        Use running mean and variance instead of batch statistics.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_features, epsilon=1e-5, momentum=0.1):\n",
        "        self.epsilon = epsilon\n",
        "        self.momentum = momentum\n",
        "\n",
        "        # Learnable parameters\n",
        "        self.gamma = tf.Variable(tf.ones(num_features), name='gamma')\n",
        "        self.beta = tf.Variable(tf.zeros(num_features), name='beta')\n",
        "\n",
        "        # Running statistics (not trainable)\n",
        "        self.running_mean = tf.Variable(tf.zeros(num_features), trainable=False)\n",
        "        self.running_var = tf.Variable(tf.ones(num_features), trainable=False)\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        if training:\n",
        "            # Compute batch statistics\n",
        "            batch_mean = tf.reduce_mean(x, axis=0)\n",
        "            batch_var = tf.math.reduce_variance(x, axis=0)\n",
        "\n",
        "            # Update running statistics\n",
        "            self.running_mean.assign(\n",
        "                (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
        "            )\n",
        "            self.running_var.assign(\n",
        "                (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
        "            )\n",
        "\n",
        "            mean, var = batch_mean, batch_var\n",
        "        else:\n",
        "            mean, var = self.running_mean, self.running_var\n",
        "\n",
        "        # Normalize\n",
        "        x_norm = (x - mean) / tf.sqrt(var + self.epsilon)\n",
        "\n",
        "        # Scale and shift\n",
        "        return self.gamma * x_norm + self.beta\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "# Test\n",
        "bn = BatchNormFromScratch(num_features=4)\n",
        "x = tf.random.normal((8, 4))  # Batch of 8, 4 features\n",
        "\n",
        "y_train = bn(x, training=True)\n",
        "y_eval = bn(x, training=False)\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"Input mean per feature: {tf.reduce_mean(x, axis=0).numpy()}\")\n",
        "print(f\"Input std per feature:  {tf.math.reduce_std(x, axis=0).numpy()}\")\n",
        "print(f\"\\nOutput (training) mean: {tf.reduce_mean(y_train, axis=0).numpy()}\")\n",
        "print(f\"Output (training) std:  {tf.math.reduce_std(y_train, axis=0).numpy()}\")\n",
        "print(f\"\\n After BatchNorm, each feature has ~0 mean and ~1 std!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR_REX3RWcC-",
        "outputId": "981fef44-0631-4260-c637-b00515caee26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        LAYER NORMALIZATION FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape: (2, 3, 4)\n",
            "\n",
            "For sample [0, 0, :]:\n",
            "  Input:  [ 0.65648675 -0.4130517   0.33997506 -1.0056272 ]\n",
            "  Output: [ 1.1744822  -0.47392502  0.6866642  -1.3872216 ]\n",
            "  Output mean: -0.000000\n",
            "  Output std:  1.0000\n",
            "\n",
            " Key difference:\n",
            "  BatchNorm: normalize across batch (for each feature)\n",
            "  LayerNorm: normalize across features (for each sample)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    LAYER NORMALIZATION FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        LAYER NORMALIZATION FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class LayerNormFromScratch:\n",
        "    \"\"\"\n",
        "    Layer Normalization: Normalize across features (not batch).\n",
        "\n",
        "    Used in Transformers because:\n",
        "    - Works with any batch size (including 1)\n",
        "    - No running statistics needed\n",
        "    - Each sample normalized independently\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, normalized_shape, epsilon=1e-5):\n",
        "        self.epsilon = epsilon\n",
        "        self.normalized_shape = normalized_shape\n",
        "\n",
        "        # Learnable parameters\n",
        "        self.gamma = tf.Variable(tf.ones(normalized_shape), name='gamma')\n",
        "        self.beta = tf.Variable(tf.zeros(normalized_shape), name='beta')\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # Compute statistics across last dimensions\n",
        "        mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
        "        var = tf.math.reduce_variance(x, axis=-1, keepdims=True)\n",
        "\n",
        "        # Normalize\n",
        "        x_norm = (x - mean) / tf.sqrt(var + self.epsilon)\n",
        "\n",
        "        # Scale and shift\n",
        "        return self.gamma * x_norm + self.beta\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "# Test\n",
        "ln = LayerNormFromScratch(normalized_shape=4)\n",
        "x = tf.random.normal((2, 3, 4))  # (batch, seq, features)\n",
        "\n",
        "y = ln(x)\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"\\nFor sample [0, 0, :]:\")\n",
        "print(f\"  Input:  {x[0, 0, :].numpy()}\")\n",
        "print(f\"  Output: {y[0, 0, :].numpy()}\")\n",
        "print(f\"  Output mean: {tf.reduce_mean(y[0, 0, :]).numpy():.6f}\")\n",
        "print(f\"  Output std:  {tf.math.reduce_std(y[0, 0, :]).numpy():.4f}\")\n",
        "\n",
        "print(f\"\\n Key difference:\")\n",
        "print(f\"  BatchNorm: normalize across batch (for each feature)\")\n",
        "print(f\"  LayerNorm: normalize across features (for each sample)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4Ok8o68WcC_",
        "outputId": "1b525f85-a4ec-4b99-d2a7-9fd6ea60b4e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "            DROPOUT FROM SCRATCH\n",
            "============================================================\n",
            "Input: all ones, shape (2, 10)\n",
            "\n",
            "Dropout sample 1: [0. 2. 2. 0. 0. 2. 0. 2. 2. 0.]\n",
            "Dropout sample 2: [0. 2. 2. 2. 0. 0. 2. 2. 2. 0.]\n",
            "Dropout sample 3: [2. 0. 2. 2. 2. 0. 0. 2. 2. 2.]\n",
            "\n",
            "During inference (training=False):\n",
            "  Output: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "\n",
            "Average over 1000 samples: 1.0045 (should be ~1.0)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    DROPOUT FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"            DROPOUT FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def dropout_from_scratch(x, rate=0.5, training=True):\n",
        "    \"\"\"\n",
        "    Dropout: Randomly zero out neurons during training.\n",
        "\n",
        "    Key insight: Scale by 1/(1-rate) during training so that\n",
        "    expected value remains the same during inference.\n",
        "    \"\"\"\n",
        "    if not training or rate == 0:\n",
        "        return x\n",
        "\n",
        "    # Create random mask\n",
        "    keep_prob = 1 - rate\n",
        "    mask = tf.cast(\n",
        "        tf.random.uniform(tf.shape(x)) < keep_prob,\n",
        "        dtype=x.dtype\n",
        "    )\n",
        "\n",
        "    # Apply mask and scale\n",
        "    return (x * mask) / keep_prob\n",
        "\n",
        "# Test\n",
        "x = tf.ones((2, 10))\n",
        "\n",
        "print(f\"Input: all ones, shape {x.shape}\")\n",
        "print(f\"\")\n",
        "\n",
        "# Multiple dropout samples\n",
        "for i in range(3):\n",
        "    dropped = dropout_from_scratch(x, rate=0.5, training=True)\n",
        "    print(f\"Dropout sample {i+1}: {dropped[0].numpy()}\")\n",
        "\n",
        "print(f\"\\nDuring inference (training=False):\")\n",
        "print(f\"  Output: {dropout_from_scratch(x, rate=0.5, training=False)[0].numpy()}\")\n",
        "\n",
        "# Verify expected value is preserved\n",
        "samples = tf.stack([dropout_from_scratch(x, rate=0.5) for _ in range(1000)])\n",
        "print(f\"\\nAverage over 1000 samples: {tf.reduce_mean(samples).numpy():.4f} (should be ~1.0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part III: Custom Layers Using Primitives\n",
        "\n",
        "## Building Layers with tf.Variable Only\n",
        "\n",
        "Before using Keras's layer system, let's build fully functional layers using only basic TensorFlow operations. This shows exactly what happens under the hood."
      ],
      "metadata": {
        "id": "4XqEZxTtWcC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    DENSE LAYER FROM PRIMITIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          DENSE LAYER FROM PRIMITIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class DenseLayerPrimitive:\n",
        "    \"\"\"\n",
        "    Fully connected layer using only tf.Variable.\n",
        "\n",
        "    Mathematically: y = activation(x @ W + b)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, activation=None, use_bias=True):\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        # Select activation\n",
        "        self.activation = {\n",
        "            None: lambda x: x,\n",
        "            'relu': tf.nn.relu,\n",
        "            'sigmoid': tf.nn.sigmoid,\n",
        "            'tanh': tf.nn.tanh,\n",
        "            'softmax': lambda x: tf.nn.softmax(x, axis=-1)\n",
        "        }.get(activation, activation)  # Allow passing functions directly\n",
        "\n",
        "        # He initialization for weights\n",
        "        stddev = np.sqrt(2.0 / in_features)\n",
        "        self.W = tf.Variable(\n",
        "            tf.random.normal((in_features, out_features), stddev=stddev),\n",
        "            trainable=True,\n",
        "            name='kernel'\n",
        "        )\n",
        "\n",
        "        if use_bias:\n",
        "            self.b = tf.Variable(\n",
        "                tf.zeros(out_features),\n",
        "                trainable=True,\n",
        "                name='bias'\n",
        "            )\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Forward pass: y = activation(x @ W + b)\"\"\"\n",
        "        out = x @ self.W\n",
        "        if self.use_bias:\n",
        "            out = out + self.b\n",
        "        return self.activation(out)\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        if self.use_bias:\n",
        "            return [self.W, self.b]\n",
        "        return [self.W]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"DenseLayerPrimitive({self.in_features}, {self.out_features})\"\n",
        "\n",
        "# Test\n",
        "dense = DenseLayerPrimitive(4, 3, activation='relu')\n",
        "x = tf.random.normal((2, 4))\n",
        "y = dense(x)\n",
        "\n",
        "print(f\"\\nDenseLayerPrimitive(4, 3, activation='relu')\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Weight shape: {dense.W.shape}\")\n",
        "print(f\"Bias shape:   {dense.b.shape}\")\n",
        "print(f\"\\nOutput:\\n{y.numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gesc0NBxWcC_",
        "outputId": "995e1b2b-35c0-4b14-dae6-50ffb422ae18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          DENSE LAYER FROM PRIMITIVES\n",
            "============================================================\n",
            "\n",
            "DenseLayerPrimitive(4, 3, activation='relu')\n",
            "Input shape:  (2, 4)\n",
            "Output shape: (2, 3)\n",
            "Weight shape: (4, 3)\n",
            "Bias shape:   (3,)\n",
            "\n",
            "Output:\n",
            "[[0.         0.         0.46504724]\n",
            " [0.         0.         0.8249154 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CONV2D LAYER FROM PRIMITIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          CONV2D LAYER FROM PRIMITIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class Conv2DLayerPrimitive:\n",
        "    \"\"\"\n",
        "    2D Convolutional layer using only tf.Variable and tf.nn.conv2d.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding='SAME', activation=None, use_bias=True):\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        # Handle kernel_size as int or tuple\n",
        "        if isinstance(kernel_size, int):\n",
        "            kernel_size = (kernel_size, kernel_size)\n",
        "\n",
        "        # Select activation\n",
        "        self.activation = {\n",
        "            None: lambda x: x,\n",
        "            'relu': tf.nn.relu,\n",
        "            'sigmoid': tf.nn.sigmoid,\n",
        "            'tanh': tf.nn.tanh,\n",
        "        }.get(activation, activation)\n",
        "\n",
        "        # He initialization\n",
        "        fan_in = kernel_size[0] * kernel_size[1] * in_channels\n",
        "        stddev = np.sqrt(2.0 / fan_in)\n",
        "\n",
        "        # Kernel shape: (height, width, in_channels, out_channels)\n",
        "        self.kernel = tf.Variable(\n",
        "            tf.random.normal((kernel_size[0], kernel_size[1], in_channels, out_channels),\n",
        "                           stddev=stddev),\n",
        "            trainable=True,\n",
        "            name='kernel'\n",
        "        )\n",
        "\n",
        "        if use_bias:\n",
        "            self.bias = tf.Variable(\n",
        "                tf.zeros(out_channels),\n",
        "                trainable=True,\n",
        "                name='bias'\n",
        "            )\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Forward pass using tf.nn.conv2d\"\"\"\n",
        "        out = tf.nn.conv2d(x, self.kernel, strides=self.stride, padding=self.padding)\n",
        "        if self.use_bias:\n",
        "            out = out + self.bias\n",
        "        return self.activation(out)\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        if self.use_bias:\n",
        "            return [self.kernel, self.bias]\n",
        "        return [self.kernel]\n",
        "\n",
        "# Test\n",
        "conv = Conv2DLayerPrimitive(3, 16, kernel_size=3, activation='relu')\n",
        "x = tf.random.normal((1, 28, 28, 3))  # 1 image, 28x28, 3 channels\n",
        "y = conv(x)\n",
        "\n",
        "print(f\"\\nConv2DLayerPrimitive(3, 16, kernel_size=3)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Kernel shape: {conv.kernel.shape}\")\n",
        "print(f\"Parameters:   {np.prod(conv.kernel.shape) + conv.bias.shape[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSWj-SXwWcC_",
        "outputId": "a74c0da8-3b3d-4049-89af-ae620a91d78c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          CONV2D LAYER FROM PRIMITIVES\n",
            "============================================================\n",
            "\n",
            "Conv2DLayerPrimitive(3, 16, kernel_size=3)\n",
            "Input shape:  (1, 28, 28, 3)\n",
            "Output shape: (1, 28, 28, 16)\n",
            "Kernel shape: (3, 3, 3, 16)\n",
            "Parameters:   448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    COMPLETE CNN FROM PRIMITIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          COMPLETE CNN FROM PRIMITIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class CNNFromPrimitives:\n",
        "    \"\"\"\n",
        "    A complete CNN built using only primitive layers.\n",
        "\n",
        "    Architecture:\n",
        "        Conv(3x3) -> ReLU -> MaxPool -> Conv(3x3) -> ReLU -> MaxPool -> Flatten -> Dense -> Dense\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        self.input_shape = input_shape\n",
        "        in_channels = input_shape[-1]\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = Conv2DLayerPrimitive(in_channels, 32, kernel_size=3, activation='relu')\n",
        "        self.conv2 = Conv2DLayerPrimitive(32, 64, kernel_size=3, activation='relu')\n",
        "\n",
        "        # Calculate flattened size after convolutions and pooling\n",
        "        # With SAME padding and 2x2 pooling twice: H/4, W/4\n",
        "        h, w = input_shape[0] // 4, input_shape[1] // 4\n",
        "        flat_size = h * w * 64\n",
        "\n",
        "        # Dense layers\n",
        "        self.fc1 = DenseLayerPrimitive(flat_size, 128, activation='relu')\n",
        "        self.fc2 = DenseLayerPrimitive(128, num_classes, activation='softmax')\n",
        "\n",
        "        self.layers = [self.conv1, self.conv2, self.fc1, self.fc2]\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        # Conv block 1\n",
        "        x = self.conv1(x)\n",
        "        x = tf.nn.max_pool2d(x, ksize=2, strides=2, padding='SAME')\n",
        "\n",
        "        # Conv block 2\n",
        "        x = self.conv2(x)\n",
        "        x = tf.nn.max_pool2d(x, ksize=2, strides=2, padding='SAME')\n",
        "\n",
        "        # Flatten\n",
        "        x = tf.reshape(x, (tf.shape(x)[0], -1))\n",
        "\n",
        "        # Dense layers\n",
        "        x = self.fc1(x)\n",
        "        if training:\n",
        "            x = dropout_from_scratch(x, rate=0.5, training=True)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        variables = []\n",
        "        for layer in self.layers:\n",
        "            variables.extend(layer.trainable_variables)\n",
        "        return variables\n",
        "\n",
        "    def summary(self):\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"        CNN FROM PRIMITIVES - SUMMARY\")\n",
        "        print(\"=\"*50)\n",
        "        total = 0\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            params = sum(np.prod(v.shape) for v in layer.trainable_variables)\n",
        "            total += params\n",
        "            print(f\"Layer {i+1}: {layer.__class__.__name__:20} | Params: {params:,}\")\n",
        "        print(\"-\"*50)\n",
        "        print(f\"Total trainable parameters: {total:,}\")\n",
        "\n",
        "# Create and test\n",
        "cnn = CNNFromPrimitives(input_shape=(28, 28, 1), num_classes=10)\n",
        "cnn.summary()\n",
        "\n",
        "# Test forward pass\n",
        "x = tf.random.normal((4, 28, 28, 1))\n",
        "y = cnn(x)\n",
        "print(f\"\\nInput shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Output sum per sample: {tf.reduce_sum(y, axis=1).numpy()}  (should be ~1.0)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvCl_LZwWcC_",
        "outputId": "6b43250c-eb4e-4eb4-e905-5a0cf2462fbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          COMPLETE CNN FROM PRIMITIVES\n",
            "============================================================\n",
            "\n",
            "==================================================\n",
            "        CNN FROM PRIMITIVES - SUMMARY\n",
            "==================================================\n",
            "Layer 1: Conv2DLayerPrimitive | Params: 320\n",
            "Layer 2: Conv2DLayerPrimitive | Params: 18,496\n",
            "Layer 3: DenseLayerPrimitive  | Params: 401,536\n",
            "Layer 4: DenseLayerPrimitive  | Params: 1,290\n",
            "--------------------------------------------------\n",
            "Total trainable parameters: 421,642\n",
            "\n",
            "Input shape:  (4, 28, 28, 1)\n",
            "Output shape: (4, 10)\n",
            "Output sum per sample: [0.9999999 1.        1.        0.9999999]  (should be ~1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part IV: Custom Keras Layers\n",
        "\n",
        "## The Proper Way to Build Custom Layers\n",
        "\n",
        "Keras provides a clean API for custom layers with:\n",
        "- **`build()`**: Create weights when input shape is known\n",
        "- **`call()`**: Define the forward pass\n",
        "- **`get_config()`**: Enable serialization\n",
        "\n",
        "This gives you all the benefits of primitives PLUS:\n",
        "- Automatic weight tracking\n",
        "- Serialization/deserialization\n",
        "- Integration with `model.fit()`\n",
        "- Proper shape inference"
      ],
      "metadata": {
        "id": "CPUeDPKIWcC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM KERAS LAYER: BASICS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          CUSTOM KERAS LAYER: BASICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class CustomDenseLayer(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Custom Dense layer demonstrating the Keras layer API.\n",
        "\n",
        "    Key methods:\n",
        "    - __init__: Store configuration (no weights yet!)\n",
        "    - build: Create weights when input shape is known\n",
        "    - call: Forward pass\n",
        "    - get_config: For serialization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units, activation=None, use_bias=True, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "        Create weights. Called automatically the first time the layer is used.\n",
        "\n",
        "        self.add_weight() creates a tf.Variable and registers it properly.\n",
        "        \"\"\"\n",
        "        self.kernel = self.add_weight(\n",
        "            name='kernel',\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer='glorot_uniform',  # Xavier initialization\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(\n",
        "                name='bias',\n",
        "                shape=(self.units,),\n",
        "                initializer='zeros',\n",
        "                trainable=True\n",
        "            )\n",
        "\n",
        "        # Mark as built\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        output = tf.matmul(inputs, self.kernel)\n",
        "        if self.use_bias:\n",
        "            output = output + self.bias\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"Enable serialization.\"\"\"\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'units': self.units,\n",
        "            'activation': keras.activations.serialize(self.activation),\n",
        "            'use_bias': self.use_bias\n",
        "        })\n",
        "        return config\n",
        "\n",
        "# Test custom layer\n",
        "custom_dense = CustomDenseLayer(32, activation='relu')\n",
        "x = tf.random.normal((4, 16))\n",
        "y = custom_dense(x)  # This triggers build()\n",
        "\n",
        "print(f\"\\nCustomDenseLayer(32, activation='relu')\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Kernel shape: {custom_dense.kernel.shape}\")\n",
        "print(f\"Trainable variables: {len(custom_dense.trainable_variables)}\")\n",
        "print(f\"\\nConfig: {custom_dense.get_config()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnKJDd05WcC_",
        "outputId": "f7f05719-bf81-41a8-e0ca-a36bfb9a7fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          CUSTOM KERAS LAYER: BASICS\n",
            "============================================================\n",
            "\n",
            "CustomDenseLayer(32, activation='relu')\n",
            "Input shape:  (4, 16)\n",
            "Output shape: (4, 32)\n",
            "Kernel shape: (16, 32)\n",
            "Trainable variables: 2\n",
            "\n",
            "Config: {'name': 'custom_dense_layer', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 32, 'activation': 'relu', 'use_bias': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM LAYER: SELF-ATTENTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"         CUSTOM LAYER: SELF-ATTENTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class SelfAttentionLayer(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Self-Attention layer (simplified version of Transformer attention).\n",
        "\n",
        "    Attention(Q, K, V) = softmax(Q @ K^T / sqrt(d_k)) @ V\n",
        "\n",
        "    In self-attention, Q, K, V all come from the same input.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads=1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Linear projections for Q, K, V\n",
        "        self.W_q = self.add_weight(\n",
        "            name='W_q',\n",
        "            shape=(input_shape[-1], self.embed_dim),\n",
        "            initializer='glorot_uniform'\n",
        "        )\n",
        "        self.W_k = self.add_weight(\n",
        "            name='W_k',\n",
        "            shape=(input_shape[-1], self.embed_dim),\n",
        "            initializer='glorot_uniform'\n",
        "        )\n",
        "        self.W_v = self.add_weight(\n",
        "            name='W_v',\n",
        "            shape=(input_shape[-1], self.embed_dim),\n",
        "            initializer='glorot_uniform'\n",
        "        )\n",
        "        self.W_o = self.add_weight(\n",
        "            name='W_o',\n",
        "            shape=(self.embed_dim, self.embed_dim),\n",
        "            initializer='glorot_uniform'\n",
        "        )\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        seq_len = tf.shape(inputs)[1]\n",
        "\n",
        "        # Linear projections\n",
        "        Q = inputs @ self.W_q  # (batch, seq, embed)\n",
        "        K = inputs @ self.W_k\n",
        "        V = inputs @ self.W_v\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        Q = tf.reshape(Q, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "        K = tf.reshape(K, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "        V = tf.reshape(V, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "\n",
        "        # Transpose to (batch, heads, seq, head_dim)\n",
        "        Q = tf.transpose(Q, [0, 2, 1, 3])\n",
        "        K = tf.transpose(K, [0, 2, 1, 3])\n",
        "        V = tf.transpose(V, [0, 2, 1, 3])\n",
        "\n",
        "        # Attention scores: Q @ K^T / sqrt(d_k)\n",
        "        scale = tf.sqrt(tf.cast(self.head_dim, tf.float32))\n",
        "        scores = tf.matmul(Q, K, transpose_b=True) / scale  # (batch, heads, seq, seq)\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            scores += (1 - mask) * -1e9\n",
        "\n",
        "        # Softmax\n",
        "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
        "\n",
        "        # Apply attention to values\n",
        "        context = tf.matmul(attention_weights, V)  # (batch, heads, seq, head_dim)\n",
        "\n",
        "        # Reshape back\n",
        "        context = tf.transpose(context, [0, 2, 1, 3])  # (batch, seq, heads, head_dim)\n",
        "        context = tf.reshape(context, (batch_size, seq_len, self.embed_dim))\n",
        "\n",
        "        # Output projection\n",
        "        output = context @ self.W_o\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'embed_dim': self.embed_dim,\n",
        "            'num_heads': self.num_heads\n",
        "        })\n",
        "        return config\n",
        "\n",
        "# Test\n",
        "attention = SelfAttentionLayer(embed_dim=64, num_heads=4)\n",
        "x = tf.random.normal((2, 10, 64))  # (batch, seq_len, embed_dim)\n",
        "output, weights = attention(x)\n",
        "\n",
        "print(f\"\\nSelfAttentionLayer(embed_dim=64, num_heads=4)\")\n",
        "print(f\"Input shape:            {x.shape}\")\n",
        "print(f\"Output shape:           {output.shape}\")\n",
        "print(f\"Attention weights shape: {weights.shape}\")\n",
        "print(f\"Trainable parameters:   {sum(np.prod(v.shape) for v in attention.trainable_variables):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfKOP_WWWcDA",
        "outputId": "941ad1f4-8955-444b-ccdc-9c48b336872e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         CUSTOM LAYER: SELF-ATTENTION\n",
            "============================================================\n",
            "\n",
            "SelfAttentionLayer(embed_dim=64, num_heads=4)\n",
            "Input shape:            (2, 10, 64)\n",
            "Output shape:           (2, 10, 64)\n",
            "Attention weights shape: (2, 4, 10, 10)\n",
            "Trainable parameters:   16,384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM LAYER: SPECTRAL NORMALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"       CUSTOM LAYER: SPECTRAL NORMALIZATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class SpectralNormalization(keras.layers.Wrapper):\n",
        "    \"\"\"\n",
        "    Spectral Normalization wrapper for layers.\n",
        "\n",
        "    Constrains the spectral norm (largest singular value) of the weight matrix.\n",
        "    Used in GANs to stabilize training.\n",
        "\n",
        "    W_normalized = W / sigma(W)\n",
        "    where sigma(W) is the largest singular value.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer, power_iterations=1, epsilon=1e-12, **kwargs):\n",
        "        super().__init__(layer, **kwargs)\n",
        "        self.power_iterations = power_iterations\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.layer.build(input_shape)\n",
        "\n",
        "        # Get the weight matrix\n",
        "        self.w = self.layer.kernel\n",
        "        w_shape = self.w.shape.as_list()\n",
        "\n",
        "        # Flatten weight to 2D for SVD\n",
        "        self.w_flat_shape = (np.prod(w_shape[:-1]), w_shape[-1])\n",
        "\n",
        "        # Initialize u vector (for power iteration)\n",
        "        self.u = self.add_weight(\n",
        "            name='u',\n",
        "            shape=(1, w_shape[-1]),\n",
        "            initializer='random_normal',\n",
        "            trainable=False\n",
        "        )\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        # Power iteration to estimate largest singular value\n",
        "        w_flat = tf.reshape(self.w, self.w_flat_shape)\n",
        "\n",
        "        u = self.u\n",
        "        for _ in range(self.power_iterations):\n",
        "            # v = W^T u / ||W^T u||\n",
        "            v = tf.matmul(u, tf.transpose(w_flat))\n",
        "            v = v / (tf.norm(v, ord='euclidean') + self.epsilon)\n",
        "\n",
        "            # u = W v / ||W v||\n",
        "            u = tf.matmul(v, w_flat)\n",
        "            u = u / (tf.norm(u, ord='euclidean') + self.epsilon)\n",
        "\n",
        "        if training:\n",
        "            self.u.assign(u)\n",
        "\n",
        "        # Spectral norm: sigma = u^T W v\n",
        "        # Corrected calculation for row vectors u (1, out_features) and v (1, in_features)\n",
        "        # The formula should be v W u^T\n",
        "        sigma = tf.matmul(tf.matmul(v, w_flat), tf.transpose(u))\n",
        "\n",
        "        # Normalize weight\n",
        "        w_normalized = self.w / sigma[0, 0]\n",
        "\n",
        "        # Manually perform the forward pass of the wrapped layer using w_normalized.\n",
        "        # This assumes the wrapped layer is a Dense layer based on the test case.\n",
        "        output = tf.matmul(inputs, w_normalized)\n",
        "\n",
        "        # If the wrapped layer has a bias and uses it, add it.\n",
        "        if hasattr(self.layer, 'bias') and self.layer.use_bias:\n",
        "            output = output + self.layer.bias\n",
        "\n",
        "        # If the wrapped layer has an activation function, apply it.\n",
        "        if hasattr(self.layer, 'activation') and self.layer.activation is not None:\n",
        "            output = self.layer.activation(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Test\n",
        "base_layer = layers.Dense(64)\n",
        "spectral_dense = SpectralNormalization(base_layer)\n",
        "x = tf.random.normal((4, 32))\n",
        "y = spectral_dense(x)\n",
        "\n",
        "print(f\"\\nSpectralNormalization(Dense(64))\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"\\n Use case: Stabilize GAN discriminator training\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFYq5xJbWcDA",
        "outputId": "2cdcafb7-1716-45ef-8b00-c3a8ee80caa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "       CUSTOM LAYER: SPECTRAL NORMALIZATION\n",
            "============================================================\n",
            "\n",
            "SpectralNormalization(Dense(64))\n",
            "Input shape:  (4, 32)\n",
            "Output shape: (4, 64)\n",
            "\n",
            " Use case: Stabilize GAN discriminator training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part V: Advanced Architectures\n",
        "\n",
        "## Building Modern Deep Learning Components\n",
        "\n",
        "Now let's build advanced architectural components used in state-of-the-art models:\n",
        "\n",
        "- **Residual Blocks** (ResNet)\n",
        "- **Squeeze-and-Excitation** (SENet)\n",
        "- **Transformer Encoder Block**\n",
        "- **Custom Normalization Layers**"
      ],
      "metadata": {
        "id": "biOHWrOFWcDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    RESIDUAL BLOCK (ResNet Style)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           RESIDUAL BLOCK (ResNet)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class ResidualBlock(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Residual Block: y = F(x) + x\n",
        "\n",
        "    The key insight: learning residual F(x) = y - x is easier than learning y directly.\n",
        "    This enables training very deep networks (100+ layers).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, filters, kernel_size=3, stride=1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.stride = stride\n",
        "\n",
        "        # Main path\n",
        "        self.conv1 = layers.Conv2D(filters, kernel_size, strides=stride,\n",
        "                                    padding='same', use_bias=False)\n",
        "        self.bn1 = layers.BatchNormalization()\n",
        "\n",
        "        self.conv2 = layers.Conv2D(filters, kernel_size, strides=1,\n",
        "                                    padding='same', use_bias=False)\n",
        "        self.bn2 = layers.BatchNormalization()\n",
        "\n",
        "        # Skip connection (identity or projection)\n",
        "        self.skip_conv = None\n",
        "        self.skip_bn = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Need projection if dimensions change\n",
        "        if input_shape[-1] != self.filters or self.stride != 1:\n",
        "            self.skip_conv = layers.Conv2D(self.filters, 1, strides=self.stride,\n",
        "                                           padding='same', use_bias=False)\n",
        "            self.skip_bn = layers.BatchNormalization()\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # Main path\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x, training=training)\n",
        "\n",
        "        # Skip connection\n",
        "        if self.skip_conv is not None:\n",
        "            skip = self.skip_conv(inputs)\n",
        "            skip = self.skip_bn(skip, training=training)\n",
        "        else:\n",
        "            skip = inputs\n",
        "\n",
        "        # Add and activate\n",
        "        return tf.nn.relu(x + skip)\n",
        "\n",
        "# Test\n",
        "res_block = ResidualBlock(64, stride=1)\n",
        "x = tf.random.normal((2, 32, 32, 64))\n",
        "y = res_block(x)\n",
        "\n",
        "print(f\"\\nResidualBlock(64)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "\n",
        "# With downsampling\n",
        "res_block_down = ResidualBlock(128, stride=2)\n",
        "y_down = res_block_down(x)\n",
        "print(f\"\\nResidualBlock(128, stride=2)\")\n",
        "print(f\"Output shape: {y_down.shape}  (spatial dims halved, channels doubled)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqnNTvJTWcDA",
        "outputId": "60ba405b-0e3e-401e-e22a-8a8014eeb89a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           RESIDUAL BLOCK (ResNet)\n",
            "============================================================\n",
            "\n",
            "ResidualBlock(64)\n",
            "Input shape:  (2, 32, 32, 64)\n",
            "Output shape: (2, 32, 32, 64)\n",
            "\n",
            "ResidualBlock(128, stride=2)\n",
            "Output shape: (2, 16, 16, 128)  (spatial dims halved, channels doubled)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    SQUEEZE-AND-EXCITATION BLOCK\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"         SQUEEZE-AND-EXCITATION BLOCK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class SqueezeExcitationBlock(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Squeeze-and-Excitation (SE) Block.\n",
        "\n",
        "    Learns channel-wise attention weights:\n",
        "    1. Squeeze: Global average pooling (H,W,C) -> (1,1,C)\n",
        "    2. Excitation: FC -> ReLU -> FC -> Sigmoid\n",
        "    3. Scale: Multiply input by attention weights\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, reduction_ratio=16, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        channels = input_shape[-1]\n",
        "        reduced_channels = max(channels // self.reduction_ratio, 1)\n",
        "\n",
        "        self.fc1 = layers.Dense(reduced_channels, activation='relu')\n",
        "        self.fc2 = layers.Dense(channels, activation='sigmoid')\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Squeeze: Global Average Pooling\n",
        "        squeezed = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)  # (B, 1, 1, C)\n",
        "\n",
        "        # Excitation\n",
        "        x = tf.reshape(squeezed, (tf.shape(inputs)[0], -1))  # (B, C)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = tf.reshape(x, (tf.shape(inputs)[0], 1, 1, -1))  # (B, 1, 1, C)\n",
        "\n",
        "        # Scale\n",
        "        return inputs * x\n",
        "\n",
        "# Test\n",
        "se_block = SqueezeExcitationBlock(reduction_ratio=16)\n",
        "x = tf.random.normal((2, 28, 28, 64))\n",
        "y = se_block(x)\n",
        "\n",
        "print(f\"\\nSqueezeExcitationBlock(reduction_ratio=16)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"\\n SE learns which channels are important for the task\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcm-_VhvWcDA",
        "outputId": "cdef7943-3559-446a-b2b8-4a3f61f16b6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         SQUEEZE-AND-EXCITATION BLOCK\n",
            "============================================================\n",
            "\n",
            "SqueezeExcitationBlock(reduction_ratio=16)\n",
            "Input shape:  (2, 28, 28, 64)\n",
            "Output shape: (2, 28, 28, 64)\n",
            "\n",
            " SE learns which channels are important for the task\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    TRANSFORMER ENCODER BLOCK\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          TRANSFORMER ENCODER BLOCK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class TransformerEncoderBlock(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Transformer Encoder Block.\n",
        "\n",
        "    Architecture:\n",
        "        x -> LayerNorm -> MultiHeadAttention -> + (residual) ->\n",
        "             LayerNorm -> FeedForward -> + (residual) -> output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "\n",
        "        # Multi-head attention\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embed_dim // num_heads\n",
        "        )\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.ffn = Sequential([\n",
        "            layers.Dense(ff_dim, activation='gelu'),\n",
        "            layers.Dropout(dropout_rate),\n",
        "            layers.Dense(embed_dim),\n",
        "            layers.Dropout(dropout_rate)\n",
        "        ])\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=False, mask=None):\n",
        "        # Pre-norm architecture (more stable)\n",
        "        # Attention block\n",
        "        x = self.layernorm1(inputs)\n",
        "        attn_output = self.attention(x, x, attention_mask=mask, training=training)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        x = inputs + attn_output  # Residual connection\n",
        "\n",
        "        # Feed-forward block\n",
        "        ffn_input = self.layernorm2(x)\n",
        "        ffn_output = self.ffn(ffn_input, training=training)\n",
        "\n",
        "        return x + ffn_output  # Residual connection\n",
        "\n",
        "# Test\n",
        "transformer_block = TransformerEncoderBlock(\n",
        "    embed_dim=64,\n",
        "    num_heads=4,\n",
        "    ff_dim=256,\n",
        "    dropout_rate=0.1\n",
        ")\n",
        "\n",
        "x = tf.random.normal((2, 20, 64))  # (batch, seq_len, embed_dim)\n",
        "y = transformer_block(x)\n",
        "\n",
        "print(f\"\\nTransformerEncoderBlock(embed_dim=64, num_heads=4, ff_dim=256)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Parameters:   {sum(np.prod(v.shape) for v in transformer_block.trainable_variables):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I-38YHnWcDA",
        "outputId": "2428e25a-76d6-4674-c388-86ef68bc6f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          TRANSFORMER ENCODER BLOCK\n",
            "============================================================\n",
            "\n",
            "TransformerEncoderBlock(embed_dim=64, num_heads=4, ff_dim=256)\n",
            "Input shape:  (2, 20, 64)\n",
            "Output shape: (2, 20, 64)\n",
            "Parameters:   49,984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part VI: Custom Training Loops\n",
        "\n",
        "## Full Control with GradientTape\n",
        "\n",
        "While `model.fit()` is convenient, custom training loops give you:\n",
        "\n",
        "- **Complete control** over the training process\n",
        "- **Custom metrics** and logging\n",
        "- **Complex training schemes** (GANs, reinforcement learning)\n",
        "- **Gradient manipulation** (clipping, accumulation)\n",
        "- **Multi-GPU/TPU** strategies"
      ],
      "metadata": {
        "id": "z13eQIQpWcDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    BASIC CUSTOM TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          BASIC CUSTOM TRAINING LOOP\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def custom_training_loop(model, train_data, val_data, epochs, learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Complete custom training loop with GradientTape.\n",
        "    \"\"\"\n",
        "    optimizer = keras.optimizers.Adam(learning_rate)\n",
        "    loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "    # Metrics\n",
        "    train_loss = keras.metrics.Mean()\n",
        "    train_acc = keras.metrics.SparseCategoricalAccuracy()\n",
        "    val_loss = keras.metrics.Mean()\n",
        "    val_acc = keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Reset metrics\n",
        "        train_loss.reset_state()\n",
        "        train_acc.reset_state()\n",
        "\n",
        "        # Training loop\n",
        "        for x_batch, y_batch in train_data:\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Forward pass\n",
        "                predictions = model(x_batch, training=True)\n",
        "                loss = loss_fn(y_batch, predictions)\n",
        "\n",
        "            # Backward pass\n",
        "            gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "            # Update metrics\n",
        "            train_loss.update_state(loss)\n",
        "            train_acc.update_state(y_batch, predictions)\n",
        "\n",
        "        # Validation loop\n",
        "        val_loss.reset_state()\n",
        "        val_acc.reset_state()\n",
        "\n",
        "        for x_batch, y_batch in val_data:\n",
        "            predictions = model(x_batch, training=False)\n",
        "            loss = loss_fn(y_batch, predictions)\n",
        "            val_loss.update_state(loss)\n",
        "            val_acc.update_state(y_batch, predictions)\n",
        "\n",
        "        # Record history\n",
        "        history['train_loss'].append(train_loss.result().numpy())\n",
        "        history['train_acc'].append(train_acc.result().numpy())\n",
        "        history['val_loss'].append(val_loss.result().numpy())\n",
        "        history['val_acc'].append(val_acc.result().numpy())\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "              f\"Train Loss: {train_loss.result():.4f} | \"\n",
        "              f\"Train Acc: {train_acc.result():.4f} | \"\n",
        "              f\"Val Loss: {val_loss.result():.4f} | \"\n",
        "              f\"Val Acc: {val_acc.result():.4f}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "print(\"\\nCustom training loop template created!\")\n",
        "print(\"Key components:\")\n",
        "print(\"  1. GradientTape for computing gradients\")\n",
        "print(\"  2. optimizer.apply_gradients() for weight updates\")\n",
        "print(\"  3. Metrics for tracking performance\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0S7sZ5woWcDA",
        "outputId": "68034be5-c4f1-4aaa-a33a-aef06dd84ff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          BASIC CUSTOM TRAINING LOOP\n",
            "============================================================\n",
            "\n",
            "Custom training loop template created!\n",
            "Key components:\n",
            "  1. GradientTape for computing gradients\n",
            "  2. optimizer.apply_gradients() for weight updates\n",
            "  3. Metrics for tracking performance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    TRAINING WITH GRADIENT CLIPPING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        TRAINING WITH GRADIENT CLIPPING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "@tf.function\n",
        "def train_step_with_clipping(model, x, y, optimizer, loss_fn, clip_norm=1.0):\n",
        "    \"\"\"\n",
        "    Single training step with gradient clipping.\n",
        "\n",
        "    Gradient clipping prevents exploding gradients in deep networks.\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(x, training=True)\n",
        "        loss = loss_fn(y, predictions)\n",
        "\n",
        "    # Compute gradients\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    # Clip gradients by global norm\n",
        "    gradients, global_norm = tf.clip_by_global_norm(gradients, clip_norm)\n",
        "\n",
        "    # Apply clipped gradients\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    return loss, global_norm\n",
        "\n",
        "print(\"Gradient Clipping Options:\")\n",
        "print(\"  - tf.clip_by_value(g, -clip, clip)  : Clip element-wise\")\n",
        "print(\"  - tf.clip_by_norm(g, clip)          : Clip each tensor by L2 norm\")\n",
        "print(\"  - tf.clip_by_global_norm(grads, clip): Clip all gradients together\")\n",
        "print(\"\\nGlobal norm is most common - maintains gradient direction!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt4KslvnWcDA",
        "outputId": "e93d19c7-8747-4617-e60b-78c609266f44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        TRAINING WITH GRADIENT CLIPPING\n",
            "============================================================\n",
            "Gradient Clipping Options:\n",
            "  - tf.clip_by_value(g, -clip, clip)  : Clip element-wise\n",
            "  - tf.clip_by_norm(g, clip)          : Clip each tensor by L2 norm\n",
            "  - tf.clip_by_global_norm(grads, clip): Clip all gradients together\n",
            "\n",
            "Global norm is most common - maintains gradient direction!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM MODEL WITH train_step()\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        CUSTOM MODEL WITH train_step()\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class CustomTrainableModel(keras.Model):\n",
        "    \"\"\"\n",
        "    Model with custom training logic built-in.\n",
        "\n",
        "    Override train_step() to customize what happens in model.fit().\n",
        "    This is the best of both worlds:\n",
        "    - Custom training logic\n",
        "    - Still use model.fit() with callbacks, validation, etc.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units_list, num_classes, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.dense_layers = []\n",
        "        for units in units_list:\n",
        "            self.dense_layers.append(layers.Dense(units, activation='relu'))\n",
        "            self.dense_layers.append(layers.Dropout(0.2))\n",
        "\n",
        "        self.output_layer = layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "        # Custom metrics\n",
        "        self.loss_tracker = keras.metrics.Mean(name='loss')\n",
        "        self.acc_tracker = keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = inputs\n",
        "        for layer in self.dense_layers:\n",
        "            x = layer(x, training=training)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        \"\"\"\n",
        "        Custom training step.\n",
        "        Called by model.fit() for each batch.\n",
        "        \"\"\"\n",
        "        x, y = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)\n",
        "            loss = keras.losses.sparse_categorical_crossentropy(y, y_pred)\n",
        "            loss = tf.reduce_mean(loss)\n",
        "\n",
        "        # Compute and apply gradients\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "        # Update metrics\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.acc_tracker.update_state(y, y_pred)\n",
        "\n",
        "        return {\n",
        "            'loss': self.loss_tracker.result(),\n",
        "            'accuracy': self.acc_tracker.result()\n",
        "        }\n",
        "\n",
        "    def test_step(self, data):\n",
        "        \"\"\"Custom evaluation step.\"\"\"\n",
        "        x, y = data\n",
        "        y_pred = self(x, training=False)\n",
        "        loss = keras.losses.sparse_categorical_crossentropy(y, y_pred)\n",
        "\n",
        "        self.loss_tracker.update_state(tf.reduce_mean(loss))\n",
        "        self.acc_tracker.update_state(y, y_pred)\n",
        "\n",
        "        return {\n",
        "            'loss': self.loss_tracker.result(),\n",
        "            'accuracy': self.acc_tracker.result()\n",
        "        }\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker, self.acc_tracker]\n",
        "\n",
        "# Test\n",
        "custom_model = CustomTrainableModel([64, 32], num_classes=10)\n",
        "custom_model.compile(optimizer='adam')\n",
        "\n",
        "print(\"\\nCustom model with train_step() created!\")\n",
        "print(\"Now model.fit() uses our custom training logic\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ocCaxVeWcDA",
        "outputId": "b187ba0c-e348-444f-dc15-4102e68e60fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        CUSTOM MODEL WITH train_step()\n",
            "============================================================\n",
            "\n",
            "Custom model with train_step() created!\n",
            "Now model.fit() uses our custom training logic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part VII: Practical Demos\n",
        "\n",
        "## Putting It All Together\n",
        "\n",
        "Let's combine everything we've learned in real examples."
      ],
      "metadata": {
        "id": "S6cCw5HyWcDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#              DEMO 1: CUSTOM RESNET FOR DIGIT CLASSIFICATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"     DEMO: CUSTOM RESNET FOR DIGIT CLASSIFICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load digits dataset\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "# Preprocess\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape for CNN: (samples, 8, 8, 1)\n",
        "X = X.reshape(-1, 8, 8, 1).astype(np.float32)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training samples: {X_train.shape[0]}\")\n",
        "print(f\"Test samples:     {X_test.shape[0]}\")\n",
        "print(f\"Image shape:      {X_train.shape[1:]}\")\n",
        "\n",
        "# Build custom ResNet model\n",
        "class MiniResNet(keras.Model):\n",
        "    \"\"\"Mini ResNet with custom residual blocks.\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initial convolution\n",
        "        self.conv1 = layers.Conv2D(32, 3, padding='same', activation='relu')\n",
        "\n",
        "        # Residual blocks\n",
        "        self.res_block1 = ResidualBlock(32)\n",
        "        self.res_block2 = ResidualBlock(64, stride=2)\n",
        "\n",
        "        # SE block for channel attention\n",
        "        self.se_block = SqueezeExcitationBlock(reduction_ratio=8)\n",
        "\n",
        "        # Classification head\n",
        "        self.global_pool = layers.GlobalAveragePooling2D()\n",
        "        self.dropout = layers.Dropout(0.3)\n",
        "        self.dense = layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        x = self.conv1(x)\n",
        "        x = self.res_block1(x, training=training)\n",
        "        x = self.res_block2(x, training=training)\n",
        "        x = self.se_block(x)\n",
        "        x = self.global_pool(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "        return self.dense(x)\n",
        "\n",
        "# Create and compile\n",
        "tf.random.set_seed(42)\n",
        "resnet_model = MiniResNet(num_classes=10)\n",
        "\n",
        "resnet_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Build model by calling it\n",
        "_ = resnet_model(X_train[:1])\n",
        "resnet_model.summary()\n",
        "\n",
        "# Train\n",
        "print(\"\\nTraining MiniResNet...\")\n",
        "history = resnet_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "test_loss, test_acc = resnet_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\nTest Accuracy: {test_acc*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hJng8MQqWcDA",
        "outputId": "6fd8ccad-f2ad-4458-848c-6697e27c50a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "     DEMO: CUSTOM RESNET FOR DIGIT CLASSIFICATION\n",
            "============================================================\n",
            "Training samples: 1437\n",
            "Test samples:     360\n",
            "Image shape:      (8, 8, 1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"mini_res_net\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"mini_res_net\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)                (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)                     \u001b[38;5;34m320\u001b[0m \n",
              "\n",
              " residual_block_2                 ?                              \u001b[38;5;34m18,688\u001b[0m \n",
              " (\u001b[38;5;33mResidualBlock\u001b[0m)                                                        \n",
              "\n",
              " residual_block_3                 ?                              \u001b[38;5;34m58,112\u001b[0m \n",
              " (\u001b[38;5;33mResidualBlock\u001b[0m)                                                        \n",
              "\n",
              " squeeze_excitation_block_1       ?                               \u001b[38;5;34m1,096\u001b[0m \n",
              " (\u001b[38;5;33mSqueezeExcitationBlock\u001b[0m)                                               \n",
              "\n",
              " global_average_pooling2d         ?                                   \u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                               \n",
              "\n",
              " dropout_6 (\u001b[38;5;33mDropout\u001b[0m)              ?                                   \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " dense_8 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m)                           \u001b[38;5;34m650\u001b[0m \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
              "\n",
              " conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> \n",
              "\n",
              " residual_block_2                 ?                              <span style=\"color: #00af00; text-decoration-color: #00af00\">18,688</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)                                                        \n",
              "\n",
              " residual_block_3                 ?                              <span style=\"color: #00af00; text-decoration-color: #00af00\">58,112</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)                                                        \n",
              "\n",
              " squeeze_excitation_block_1       ?                               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,096</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SqueezeExcitationBlock</span>)                                               \n",
              "\n",
              " global_average_pooling2d         ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                               \n",
              "\n",
              " dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m78,866\u001b[0m (308.07 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">78,866</span> (308.07 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m78,354\u001b[0m (306.07 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">78,354</span> (306.07 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m512\u001b[0m (2.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> (2.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training MiniResNet...\n",
            "Epoch 1/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 49ms/step - accuracy: 0.3334 - loss: 2.0227 - val_accuracy: 0.4028 - val_loss: 2.2140\n",
            "Epoch 2/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.8850 - loss: 0.8024 - val_accuracy: 0.3083 - val_loss: 2.0655\n",
            "Epoch 3/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9588 - loss: 0.3152 - val_accuracy: 0.4278 - val_loss: 1.8874\n",
            "Epoch 4/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9817 - loss: 0.1319 - val_accuracy: 0.5556 - val_loss: 1.5522\n",
            "Epoch 5/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9969 - loss: 0.0694 - val_accuracy: 0.6028 - val_loss: 1.3664\n",
            "Epoch 6/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9989 - loss: 0.0447 - val_accuracy: 0.6528 - val_loss: 1.0092\n",
            "Epoch 7/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0307 - val_accuracy: 0.8722 - val_loss: 0.4159\n",
            "Epoch 8/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9981 - loss: 0.0230 - val_accuracy: 0.9667 - val_loss: 0.1579\n",
            "Epoch 9/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9965 - loss: 0.0218 - val_accuracy: 0.9722 - val_loss: 0.1018\n",
            "Epoch 10/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0131 - val_accuracy: 0.9833 - val_loss: 0.0509\n",
            "Epoch 11/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0096 - val_accuracy: 0.9917 - val_loss: 0.0353\n",
            "Epoch 12/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0086 - val_accuracy: 0.9917 - val_loss: 0.0301\n",
            "Epoch 13/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0083 - val_accuracy: 0.9917 - val_loss: 0.0355\n",
            "Epoch 14/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0096 - val_accuracy: 0.9917 - val_loss: 0.0313\n",
            "Epoch 15/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0075 - val_accuracy: 0.9917 - val_loss: 0.0304\n",
            "Epoch 16/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0070 - val_accuracy: 0.9917 - val_loss: 0.0222\n",
            "Epoch 17/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0066 - val_accuracy: 0.9917 - val_loss: 0.0280\n",
            "Epoch 18/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0050 - val_accuracy: 0.9944 - val_loss: 0.0241\n",
            "Epoch 19/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0065 - val_accuracy: 0.9889 - val_loss: 0.0426\n",
            "Epoch 20/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.9917 - val_loss: 0.0206\n",
            "Epoch 21/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.9917 - val_loss: 0.0214\n",
            "Epoch 22/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 0.9917 - val_loss: 0.0207\n",
            "Epoch 23/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 0.9917 - val_loss: 0.0239\n",
            "Epoch 24/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.9917 - val_loss: 0.0192\n",
            "Epoch 25/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.9944 - val_loss: 0.0255\n",
            "Epoch 26/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.9917 - val_loss: 0.0229\n",
            "Epoch 27/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.9889 - val_loss: 0.0274\n",
            "Epoch 28/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.9917 - val_loss: 0.0258\n",
            "Epoch 29/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.9944 - val_loss: 0.0238\n",
            "Epoch 30/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.9917 - val_loss: 0.0374\n",
            "\n",
            "Test Accuracy: 99.17%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    VISUALIZE TRAINING RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "# Plot training history\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax1.plot(history.history['loss'], 'b-', label='Train Loss')\n",
        "ax1.plot(history.history['val_loss'], 'r-', label='Val Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('MiniResNet Training Loss', fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.plot(history.history['accuracy'], 'b-', label='Train Acc')\n",
        "ax2.plot(history.history['val_accuracy'], 'r-', label='Val Acc')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('MiniResNet Training Accuracy', fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize predictions\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "fig.suptitle('MiniResNet Predictions', fontsize=14, fontweight='bold')\n",
        "\n",
        "predictions = resnet_model.predict(X_test[:10], verbose=0)\n",
        "pred_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    img = X_test[i].reshape(8, 8)\n",
        "    true_label = y_test[i]\n",
        "    pred_label = pred_classes[i]\n",
        "    confidence = predictions[i][pred_label] * 100\n",
        "\n",
        "    ax.imshow(img, cmap='gray')\n",
        "    color = 'green' if true_label == pred_label else 'red'\n",
        "    ax.set_title(f'True: {true_label}, Pred: {pred_label}\\n({confidence:.1f}%)',\n",
        "                 color=color, fontsize=10)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        },
        "id": "6QkXz-l8WcDA",
        "outputId": "a94e76fc-263f-4107-96bd-c6fc0ca2badf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x600 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                         ADVANCED CHEAT SHEET\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"              ADVANCED TENSORFLOW/KERAS CHEAT SHEET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "cheat_sheet = \"\"\"\n",
        "GRADIENTTAPE PATTERNS\n",
        "---------------------\n",
        "# Basic gradient\n",
        "with tf.GradientTape() as tape:\n",
        "    y = model(x)\n",
        "grads = tape.gradient(y, model.trainable_variables)\n",
        "\n",
        "# Higher-order derivatives (nested tapes)\n",
        "with tf.GradientTape() as t2:\n",
        "    with tf.GradientTape() as t1:\n",
        "        y = f(x)\n",
        "    dy = t1.gradient(y, x)\n",
        "d2y = t2.gradient(dy, x)\n",
        "\n",
        "# Jacobian\n",
        "jacobian = tape.jacobian(y, x)\n",
        "\n",
        "# Custom gradient\n",
        "@tf.custom_gradient\n",
        "def custom_op(x):\n",
        "    def grad(dy):\n",
        "        return dy * custom_backward\n",
        "    return forward_result, grad\n",
        "\n",
        "CUSTOM KERAS LAYERS\n",
        "-------------------\n",
        "class CustomLayer(keras.layers.Layer):\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True\n",
        "        )\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        return inputs @ self.kernel\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config['units'] = self.units\n",
        "        return config\n",
        "\n",
        "CUSTOM TRAINING\n",
        "---------------\n",
        "# Override train_step for model.fit()\n",
        "class CustomModel(keras.Model):\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)\n",
        "            loss = self.compute_loss(y=y, y_pred=y_pred)\n",
        "        grads = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "        return {'loss': loss}\n",
        "\n",
        "GRADIENT MANIPULATION\n",
        "---------------------\n",
        "# Clip by global norm\n",
        "grads, _ = tf.clip_by_global_norm(grads, max_norm=1.0)\n",
        "\n",
        "# Gradient accumulation\n",
        "accumulated = [acc + g/steps for acc, g in zip(accumulated, grads)]\n",
        "\"\"\"\n",
        "print(cheat_sheet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5Z_iXlFWcDA",
        "outputId": "9658683d-7dc7-4242-ed9e-10589cedd91b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "              ADVANCED TENSORFLOW/KERAS CHEAT SHEET\n",
            "======================================================================\n",
            "\n",
            "GRADIENTTAPE PATTERNS\n",
            "---------------------\n",
            "# Basic gradient\n",
            "with tf.GradientTape() as tape:\n",
            "    y = model(x)\n",
            "grads = tape.gradient(y, model.trainable_variables)\n",
            "\n",
            "# Higher-order derivatives (nested tapes)\n",
            "with tf.GradientTape() as t2:\n",
            "    with tf.GradientTape() as t1:\n",
            "        y = f(x)\n",
            "    dy = t1.gradient(y, x)\n",
            "d2y = t2.gradient(dy, x)\n",
            "\n",
            "# Jacobian\n",
            "jacobian = tape.jacobian(y, x)\n",
            "\n",
            "# Custom gradient\n",
            "@tf.custom_gradient\n",
            "def custom_op(x):\n",
            "    def grad(dy):\n",
            "        return dy * custom_backward\n",
            "    return forward_result, grad\n",
            "\n",
            "CUSTOM KERAS LAYERS\n",
            "-------------------\n",
            "class CustomLayer(keras.layers.Layer):\n",
            "    def __init__(self, units, **kwargs):\n",
            "        super().__init__(**kwargs)\n",
            "        self.units = units\n",
            "\n",
            "    def build(self, input_shape):\n",
            "        self.kernel = self.add_weight(\n",
            "            shape=(input_shape[-1], self.units),\n",
            "            initializer='glorot_uniform',\n",
            "            trainable=True\n",
            "        )\n",
            "        super().build(input_shape)\n",
            "\n",
            "    def call(self, inputs, training=False):\n",
            "        return inputs @ self.kernel\n",
            "\n",
            "    def get_config(self):\n",
            "        config = super().get_config()\n",
            "        config['units'] = self.units\n",
            "        return config\n",
            "\n",
            "CUSTOM TRAINING\n",
            "---------------\n",
            "# Override train_step for model.fit()\n",
            "class CustomModel(keras.Model):\n",
            "    def train_step(self, data):\n",
            "        x, y = data\n",
            "        with tf.GradientTape() as tape:\n",
            "            y_pred = self(x, training=True)\n",
            "            loss = self.compute_loss(y=y, y_pred=y_pred)\n",
            "        grads = tape.gradient(loss, self.trainable_variables)\n",
            "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
            "        return {'loss': loss}\n",
            "\n",
            "GRADIENT MANIPULATION\n",
            "---------------------\n",
            "# Clip by global norm\n",
            "grads, _ = tf.clip_by_global_norm(grads, max_norm=1.0)\n",
            "\n",
            "# Gradient accumulation\n",
            "accumulated = [acc + g/steps for acc, g in zip(accumulated, grads)]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "## Your Advanced TensorFlow & Keras Journey\n",
        "\n",
        "Congratulations! You've mastered advanced TensorFlow and Keras techniques.\n",
        "\n",
        "### What You Learned\n",
        "\n",
        "| Part | Topic | Key Takeaway |\n",
        "|------|-------|-------------|\n",
        "| I | Advanced GradientTape | Nested tapes, Jacobians, custom gradients |\n",
        "| II | Building Ops | Conv, pooling, normalization from scratch |\n",
        "| III | Primitive Layers | Dense, Conv2D with only tf.Variable |\n",
        "| IV | Custom Keras Layers | Proper subclassing with build() and call() |\n",
        "| V | Advanced Architectures | ResNet, SE-Net, Transformer blocks |\n",
        "| VI | Custom Training | Full control with GradientTape |\n",
        "| VII | Practical Demos | Real-world model combining everything |\n",
        "\n",
        "### When to Use What\n",
        "\n",
        "| Approach | Use When |\n",
        "|----------|----------|\n",
        "| `model.fit()` | Standard training, quick prototyping |\n",
        "| Custom `train_step()` | Custom logic but want callbacks/validation |\n",
        "| Full GradientTape loop | GANs, RL, complex multi-model training |\n",
        "| Custom layers | Reusable components, research |\n",
        "| Primitive layers | Learning, debugging, maximum control |\n",
        "\n",
        "### The Complete Learning Path\n",
        "\n",
        "1. **NumPy from Scratch** - Understand the math deeply\n",
        "2. **PyTorch** - Research-friendly framework\n",
        "3. **TensorFlow/Keras Part 1** - Fundamentals and high-level API\n",
        "4. **TensorFlow/Keras Part 2** - Advanced custom components (This notebook!)\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- **Vision Transformers (ViT)** - Transformers for images\n",
        "- **Diffusion Models** - State-of-the-art generative AI\n",
        "- **Neural Architecture Search** - Automated model design\n",
        "- **Quantization & Pruning** - Model optimization for deployment\n",
        "- **TensorFlow Extended (TFX)** - Production ML pipelines\n",
        "\n",
        "---\n",
        "\n",
        "*\"The more you understand the primitives, the better you can innovate.\"*\n",
        "\n",
        "**Happy Deep Learning!**"
      ],
      "metadata": {
        "id": "wQs6OOHGWcDA"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}